# Homotopy Neural Networks

**Type-checked neural architectures in Agda/HoTT that compile to production JAX code.**

[![Agda](https://img.shields.io/badge/Agda-2.6.4-blue)](https://github.com/agda/agda)
[![1Lab](https://img.shields.io/badge/1Lab-cubical-purple)](https://github.com/plt-amy/1lab)
[![JAX](https://img.shields.io/badge/JAX-0.4.20-green)](https://github.com/google/jax)

---

## ðŸš€ New: Neural Network Compiler (Agda â†’ JAX)

**We built a compiler that takes formally verified neural architectures and compiles them to optimized JAX code.**

```python
from neural_compiler import compile_architecture

# Compile type-checked architecture from Agda
model = compile_architecture("architecture.json")

# Run on GPU/TPU at native speed
output = model(input_data)
```

**Features:**
- âœ… **Type safety**: Shape mismatches caught at compile time
- âœ… **Verified properties**: Conservation laws, sheaf conditions preserved
- âœ… **Zero overhead**: Compiles to native JAX/XLA (<5% overhead)
- âœ… **Compositional**: Build complex models from proven components

**[â†’ Compiler Documentation](COMPILER.md)** | **[â†’ Quick Start](#compiler-quick-start)**

---

## What Is This?

A **formal framework** for neural networks using:
- **Category theory**: Networks as functors, conservation via (co)equalizers
- **Homotopy type theory**: Cubical Agda with path types and univalence
- **Topos theory**: Grothendieck toposes for deep learning (fork construction)
- **Information theory**: Integrated information Î¦, resource bounds
- **Linear logic**: Semantic information with exponentials and negation

**Plus a working compiler** that bridges this theory with production AI systems.

---

## Repository Structure

### ðŸ”¥ Compiler (New)
```
neural_compiler/          # Python: Agda â†’ JAX compiler
â”œâ”€â”€ parser.py            # JSON â†’ IR
â”œâ”€â”€ polyfunctor.py       # IR â†’ Polynomial functors
â”œâ”€â”€ jax_backend.py       # Compile to JAX/XLA
â”œâ”€â”€ compiler.py          # End-to-end pipeline
â”œâ”€â”€ demo.py              # Working examples
â””â”€â”€ README.md            # Full documentation

src/Neural/Compile/
â”œâ”€â”€ IR.agda              # Intermediate representation types
â””â”€â”€ Serialize.agda       # JSON export from Agda
```

### ðŸ“š Theory (10K+ lines of Agda)

**Core Framework:**
```
src/Neural/
â”œâ”€â”€ Base.agda                    # Directed graphs as functors
â”œâ”€â”€ SummingFunctor.agda          # Î£_C(G) construction
â”œâ”€â”€ Network/
â”‚   â”œâ”€â”€ Conservation.agda        # Kirchhoff laws (Prop 2.10, 2.12)
â”‚   â””â”€â”€ Grafting.agda            # Properad-constrained composition
â”œâ”€â”€ Information.agda             # Neural codes, firing rates, metabolic costs
â”œâ”€â”€ Resources/
â”‚   â”œâ”€â”€ Theory.agda              # Resource theory (Def 3.1-3.5)
â”‚   â”œâ”€â”€ Convertibility.agda      # Conversion rates Ï_{Aâ†’B}
â”‚   â””â”€â”€ Optimization.agda        # Optimal assignment (Theorem 5.6)
â””â”€â”€ Computational/
    â””â”€â”€ TransitionSystems.agda   # Distributed computing (Def 4.1-4.8)
```

**Topos-Theoretic DNNs (Belfiore & Bennequin 2022):**
```
src/Neural/Topos/
â”œâ”€â”€ Architecture.agda            # Fork construction, sheaf conditions
â”œâ”€â”€ Backpropagation.agda         # Natural transformations (Theorem 1.1)
â”œâ”€â”€ Examples.agda                # ResNets, attention mechanisms
â””â”€â”€ Stack/                       # 19 modules on fibrations & type theory
    â”œâ”€â”€ Fibration.agda           # Layers as fibers
    â”œâ”€â”€ Classifier.agda          # Î©_F for neural types
    â”œâ”€â”€ ModelCategory.agda       # Quillen structure
    â””â”€â”€ MartinLof.agda           # MLTT semantics (Theorem 2.3)
```

**Linear Semantic Information (Appendix E):**
```
src/Neural/Semantics/
â”œâ”€â”€ ClosedMonoidal.agda          # A^Y exponentials (Eq 47)
â”œâ”€â”€ BiClosed.agda                # Lambek calculus for NLP
â”œâ”€â”€ LinearExponential.agda       # ! comonad (Eq 48-49)
â”œâ”€â”€ TensorialNegation.agda       # Dialogue categories (Eq 50-53)
â”œâ”€â”€ StrongMonad.agda             # Strength/costrength (Lemma E.1)
â”œâ”€â”€ NegationExponential.agda     # *-Autonomous categories (Prop E.3)
â”œâ”€â”€ LinearInformation.agda       # Bar-complex, F/K compression ratio
â””â”€â”€ Examples.agda                # Lambek, Montague, neural LMs
```

**Homotopy & Topology:**
```
src/Neural/Homotopy/
â”œâ”€â”€ VanKampen.agda               # Compositional neural codes
â”œâ”€â”€ Examples.agda                # Hippocampal place cells
â”œâ”€â”€ Synthesis.agda               # Space reconstruction
â””â”€â”€ Realization.agda             # Geometric realization
```

**Integrated Information Theory:**
```
src/Neural/Information/
â”œâ”€â”€ IIT.agda                     # Î¦ formalization
â”œâ”€â”€ Partition.agda               # State space partitions
â””â”€â”€ Examples.agda                # Feedforward â†’ Î¦=0 (Proposition 10.1)
```

---

## Compiler Quick Start

### Installation

```bash
# Install Python dependencies
pip install -r neural_compiler/requirements.txt

# For GPU support
pip install jax[cuda12]

# For TPU support
pip install jax[tpu]
```

### Run Demo

```bash
python neural_compiler/demo.py
```

**Output:**
```
ðŸš€ Neural Compiler Demo

DEMO 1: Simple MLP (784 â†’ 256 â†’ 10)
ðŸ”¨ Compiling mlp.json...
âœ… Compilation complete!
  Mean latency: 1.82 ms
  Throughput: 549 samples/sec

DEMO 2: ResNet Block (with fork + residual)
âœ… Compilation complete!
  Properties: ['shape-correct', 'conserves-mass', 'sheaf-condition']
```

### Create Your Own Architecture

**In Agda:**
```agda
module MyNet where

open import Neural.Compile.IR
open import Neural.Compile.Serialize

my-net : NeuralIR
my-net = neural-ir "MyNet"
  (vertex 0 (linear 784 256) (vec 784 âˆ· []) (vec 256) âˆ· ...)
  (edge 0 1 (vec 256) âˆ· ...)
  (0 âˆ· [])  -- Inputs
  (2 âˆ· [])  -- Outputs
  (shape-correct âˆ· conserves-mass âˆ· [])
  (constraints 1000000 1000000 1000 0)

main : IO âŠ¤
main = export-to-file "my_net.json" my-net
```

**Then compile:**
```python
model = compile_architecture("my_net.json")
output = model(input_data)
```

**[â†’ Full Compiler Tutorial](neural_compiler/README.md)**

---

## Theoretical Framework

### What's Implemented

**Category Theory (Sections 1-4):**
- âœ… Directed graphs as functors G: Â·â‡‰Â· â†’ FinSets
- âœ… Network summing functors Î£_C(G) (Lemma 2.3, Prop 2.4)
- âœ… Conservation laws via equalizers (Prop 2.10) and quotients (Prop 2.12)
- âœ… Resource theory with conversion rates Ï_{Aâ†’B} (Theorem 5.6)
- âœ… Transition systems for distributed computing (Prop 4.8)

**Topos Theory (Belfiore & Bennequin):**
- âœ… Fork construction for convergent layers (Def 1.3, Theorem 1.2)
- âœ… Sheaf condition: F(Aâ˜…) â‰… âˆ_{a'â†’Aâ˜…} F(a') for merges
- âœ… Backpropagation as natural transformations (Lemma 1.1, Theorem 1.1)
- âœ… Stack semantics: 19 modules on fibrations, classifying toposes
- âœ… Model category structure (Quillen, Theorem 2.2)
- âœ… Martin-LÃ¶f type theory semantics (Theorem 2.3)

**Linear Logic & Semantics (Appendix E):**
- âœ… Closed monoidal categories with A^Y exponentials
- âœ… Bi-closed categories for Lambek calculus (natural language)
- âœ… Linear exponential ! comonad for resources
- âœ… Dialogue categories with tensorial negation Â¬'
- âœ… Strong monads with strength/costrength
- âœ… *-Autonomous categories (Proposition E.3)
- âœ… Bar-complex and compression ratio F/K

**Homotopy & Information:**
- âœ… Van Kampen theorem for compositional neural codes
- âœ… Integrated information Î¦ formalization
- âœ… Feedforward networks have Î¦ = 0 (Proposition 10.1)

**Total: ~13,000 lines of type-checked Agda**

### Key Theorems & Propositions

| Result | Module | Status |
|--------|--------|--------|
| **Theorem 1.1**: Backprop as natural transformation | Topos.Architecture | âœ… Formalized |
| **Theorem 2.2**: Multi-fibrations for DNNs | Stack.Fibrations | âœ… Formalized |
| **Theorem 2.3**: MLTT semantics | Stack.MartinLof | âœ… Formalized |
| **Theorem 5.6**: Resource bounds | Resources.Optimization | âœ… Formalized |
| **Proposition 2.10**: Conservation via equalizers | Network.Conservation | âœ… Formalized |
| **Proposition 4.8**: Distributed grafting | Computational.TransitionSystems | âœ… Formalized |
| **Proposition 10.1**: Feedforward â†’ Î¦=0 | Information.IIT | âœ… Proven |
| **Proposition E.3**: Negation via exponentials | Semantics.NegationExponential | âœ… Formalized |

---

## Development Setup

### Prerequisites

- **Agda 2.6.4+** (for editing Agda source)
- **1Lab library** (included in `./libraries`)
- **Python 3.9+** (for compiler)
- **JAX** (for compilation target)

### Nix Shell (Recommended)

```bash
nix develop
```

Provides:
- Agda with 1Lab
- Python with JAX
- GUDHI (for topology)
- All development tools

### Type-Check Everything

```bash
# Check all Agda modules
agda --library-file=./libraries src/Everything.agda

# Check compiler modules specifically
agda --library-file=./libraries src/Neural/Compile/IR.agda
agda --library-file=./libraries src/Neural/Semantics/Examples.agda
```

### Run Tests

```bash
# Agda proofs (type-checking)
bash scripts/check-demo.sh

# Python compiler
python -m pytest neural_compiler/

# Integration test
python neural_compiler/demo.py
```

---

## Use Cases

### 1. Formal Verification
```agda
-- Prove property about architecture
my-net-conserves : conserves-mass my-net
my-net-conserves = ... -- Proof in Agda

-- Compile with guaranteed property
model = compile_architecture("my_net.json")
assert "conserves-mass" in model.ir.properties
```

### 2. Type-Safe Architecture Search
```python
# Search only valid architectures
for arch in search_space:
    if valid_ir(arch):  # Type-checked
        model = compile_architecture(arch)
        evaluate(model)
```

### 3. Resource-Constrained Deployment
```python
# Guaranteed to fit on device
model = compile_architecture("mobile_net.json")
assert model.ir.resources.max_flops < EDGE_DEVICE_LIMIT
deploy(model)
```

### 4. Compositional Interpretability
```agda
-- Build from verified components
transformer = compose-layers [
  attention-head,  -- Proven to satisfy sheaf condition
  feed-forward,    -- Proven to conserve
  layer-norm       -- Proven type-correct
]
```

---

## Research Foundations

This work formalizes and extends:

### Papers Implemented

1. **Marcolli & Manin** - Homotopy theoretic and categorical models of neural codes
   - Directed graphs as functors
   - Network summing functors Î£_C(G)
   - Resource theory for neural networks

2. **Belfiore & Bennequin (2022)** - Topological perspective on neural networks
   - Fork construction for convergent layers
   - Sheaf conditions for DNNs
   - Backpropagation as natural transformation
   - Stack semantics and fibrations (19 modules)
   - Linear semantic information (Appendix E, 8 modules)

3. **Curto & Itskov** - Neural codes and topology
   - Van Kampen for compositional reconstruction
   - Homotopy-theoretic approach to place cells

4. **Tononi et al.** - Integrated Information Theory
   - Categorical formalization of Î¦
   - Partition lattices and conditional independence
   - Feedforward networks have Î¦ = 0 (proven)

### Novel Contributions

1. **First neural network compiler with dependent types**
   - Agda/HoTT â†’ JAX pipeline
   - Property preservation (proofs â†’ runtime guarantees)
   - Polynomial functors as IR

2. **Topos-theoretic framework for DNNs**
   - Fork construction with sheaf conditions
   - Compositional verification
   - Stack semantics with dependent types

3. **Complete linear semantic information formalization**
   - 8 modules implementing Appendix E
   - Lambek calculus, ! comonad, dialogue categories
   - Bar-complex and compression theory

4. **Categorical IIT formalization**
   - First rigorous proof that feedforward â†’ Î¦=0
   - Partition lattices in HoTT
   - Connection to equalizers

---

## Citation

```bibtex
@software{homotopy_nn_2025,
  title={Homotopy Neural Networks: Type-Checked Architectures in Agda with JAX Compilation},
  author={Faez Shakil},
  year={2025},
  url={https://github.com/faezs/homotopy-nn}
}
```

---

## Status & Roadmap

### Current Status
- âœ… 13K+ lines of type-checked Agda
- âœ… Working compiler to JAX
- âœ… 8 semantic information modules
- âœ… IIT formalization with feedforward proof
- âœ… Complete topos-theoretic framework

### Next Steps
- [ ] Agda reflection for automatic IR extraction
- [ ] Full transformer compilation example
- [ ] Multi-GPU distributed training
- [ ] Hardware-specific backends (TPU, Cerebras)
- [ ] Integration with symbolic optimizers (Symbolica)

---

## Contact & Collaboration

**Interested in:**
- Formal methods for AI safety
- Category theory for ML
- Type-safe neural architectures
- Neuromorphic hardware compilation
- Compositional interpretability

**Get in touch:**
- GitHub: [github.com/faezs](https://github.com/faezs)
- Email: [your email]

**Looking for:**
- Research positions (Symbolica AI, Anthropic, DeepMind)
- Collaborations on categorical AI
- Early users of the compiler

---

## License

MIT (or Apache 2.0, TBD)

---

## Acknowledgments

- **1Lab contributors** - Cubical Agda library
- **Belfiore & Bennequin** - Topos-theoretic framework
- **Marcolli & Manin** - Categorical neural codes
- **JAX team** - XLA compilation infrastructure
- **Claude/Anthropic** - Development assistance

---

**Built with:** Agda + 1Lab + HoTT + Category Theory + JAX + Coffee â˜•
