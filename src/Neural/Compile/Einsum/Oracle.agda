{-# OPTIONS --cubical #-}

{-|
# Einsum Execution Oracle via PyTorch

**Trust Boundary Architecture**:
- âœ… Verified in Agda: Type safety, index tracking, optimization correctness
- ðŸ”’ Trusted external oracle: PyTorch's `torch.einsum` implementation
- ðŸ”Œ Realized via extraction: Agda postulate â†’ Haskell FFI â†’ Python subprocess

## Key Design

**Postulated Execution**: We postulate `eval-pytorch` as an opaque function that
executes einsum formulas. This postulate is:
1. **Not axiomatically justified** - We trust PyTorch's correctness
2. **Realized via extraction** - Haskell code provides FFI implementation
3. **External to type system** - No computational rules needed

**Trust Model**: "Trust PyTorch, verify everything else"
- Agda verifies: Einsum construction, string conversion, optimization
- PyTorch handles: GPU execution, numerical precision, performance

## Example

```agda
-- Matrix multiply: A[i,j] Ã— B[j,k] â†’ C[i,k]
test : Tensor
test = evalEinsum matmul
  [ mkTensor (i âˆ· j âˆ· []) [ 1.0 , 2.0 , 3.0 , 4.0 ]  -- 2Ã—2 A
  , mkTensor (j âˆ· k âˆ· []) [ 5.0 , 6.0 , 7.0 , 8.0 ]  -- 2Ã—2 B
  ]
-- Result: mkTensor (i âˆ· k âˆ· []) [ 19.0 , 22.0 , 43.0 , 50.0 ]
```

## Extraction

In Haskell backend:
```haskell
evalPytorch :: String -> [Tensor] -> IO Tensor
evalPytorch formula tensors = do
  session <- getPythonSession
  sendJSON session $ object ["formula" .= formula, "tensors" .= tensors]
  receiveTensor session
```

-}

module Neural.Compile.Einsum.Oracle where

open import 1Lab.Prelude
open import 1Lab.Path

open import Data.List using (List; []; _âˆ·_; length)
open import Data.String.Base using (String)
open import Neural.Compile.Einsum.Index
open import Neural.Compile.Einsum.Expression

--------------------------------------------------------------------------------
-- Â§ 1: Numeric Types
--------------------------------------------------------------------------------

{-|
## Floating Point Numbers

Postulated for now - could be refined to IEEE 754 later.
-}

postulate
  Float : Type
  Float-is-set : is-set Float

--------------------------------------------------------------------------------
-- Â§ 2: Tensor Type (Concrete)
--------------------------------------------------------------------------------

{-|
## Tensor: Shaped Multi-dimensional Array

Concrete representation as shape + flattened data.

**Design choice**: Store data as flat list, not nested structure
- Matches PyTorch's internal representation
- Easy to serialize/deserialize
- Size invariant: length data â‰¡ product of shape dimensions

**TODO**: Add size invariant as field
-}

record Tensor : Type where
  constructor mkTensor
  field
    shape : IndexCtx            -- Dimension indices [batch, seq, hidden]
    elements : List Float       -- Flattened elements in row-major order

open Tensor public

-- Size of a tensor (number of elements)
tensor-size : Tensor â†’ Nat
tensor-size tens = length (tens .elements)

-- Rank of a tensor (number of dimensions)
tensor-rank : Tensor â†’ Nat
tensor-rank tens = length (tens .shape)

--------------------------------------------------------------------------------
-- Â§ 3: PyTorch Oracle (Postulated)
--------------------------------------------------------------------------------

{-|
## Execution Oracle

**POSTULATED** - No computational rules, no verification.

This is our **trust boundary**:
- Agda verifies the *formula* is type-safe
- PyTorch computes the *result*

**Implementation** (Haskell extraction):
```haskell
evalPytorch :: String -> [Tensor] -> IO Tensor
evalPytorch formula tensors = do
  -- Start persistent Python session
  session <- getOrCreatePythonSession

  -- Send: {"formula": "ij,jk->ik", "tensors": [...]}
  let request = object
        [ "formula" .= formula
        , "tensors" .= map tensorToJSON tensors
        ]
  hPutStrLn (pyStdin session) (encode request)

  -- Receive: {"shape": [2,3], "data": [1.0,2.0,...]}
  response <- hGetLine (pyStdout session)
  return $ tensorFromJSON $ decode response
```

**Type signature**:
- Input: Einsum formula as string (e.g., `"ij,jk->ik"`)
- Input: List of tensors matching formula's input contexts
- Output: Result tensor matching formula's output context

**Properties** (trusted, not proven):
1. **Type safety**: Output shape matches einsum output context
2. **Correctness**: Implements Einstein summation convention
3. **Determinism**: Same inputs â†’ same output
4. **Termination**: Always terminates (no infinite loops)

**Non-properties** (not guaranteed):
- Numerical precision (floating point rounding)
- Performance (implementation-dependent)
- Memory efficiency (may allocate large intermediates)
-}

postulate
  eval-pytorch : String â†’ List Tensor â†’ Tensor

--------------------------------------------------------------------------------
-- Â§ 4: Smart Wrapper
--------------------------------------------------------------------------------

{-|
## Verified Wrapper

Convert well-typed Einsum expression to string formula, then evaluate.

**Two-stage verification**:
1. **Type safety** (Agda): Einsum construction is well-typed
2. **String conversion** (Agda): Formula correctly represents Einsum
3. **Execution** (PyTorch - trusted): Compute result

**Example**:
```agda
evalEinsum matmul [A, B]
  â‰¡ eval-pytorch "ij,jk->ik" [A, B]
```
-}

-- Forward reference to toString function (defined in ToString.agda)
postulate
  einsumToString : {ins : List IndexCtx} {out : IndexCtx}
                 â†’ Einsum ins out
                 â†’ String

-- Smart wrapper combining type safety + execution
evalEinsum : {ins : List IndexCtx} {out : IndexCtx}
           â†’ Einsum ins out
           â†’ List Tensor
           â†’ Tensor
evalEinsum expr tensors = eval-pytorch (einsumToString expr) tensors

--------------------------------------------------------------------------------
-- Â§ 5: Examples (Placeholders)
--------------------------------------------------------------------------------

{- TODO: Uncomment when SmartConstructors is fixed
module OracleExamples where
  open SmartConstructors

  -- These will execute when we have the full pipeline
  postulate
    example-A : Tensor  -- 2Ã—3 matrix
    example-B : Tensor  -- 3Ã—4 matrix

  -- Matrix multiply via oracle
  example-matmul : Tensor
  example-matmul = evalEinsum matmul (example-A âˆ· example-B âˆ· [])

  -- Expected: 2Ã—4 matrix
  -- (This would be verified by running the extracted Haskell code)
-}

--------------------------------------------------------------------------------
-- Â§ 6: Future Work
--------------------------------------------------------------------------------

{-|
## Potential Extensions

1. **Tensor validation**: Add predicate `valid-tensor : Tensor â†’ Type`
   - Check length data â‰¡ product of dimensions
   - Use in eval-pytorch precondition

2. **Dtype support**: Parameterize Tensor by dtype (Float32, Int64, etc.)
   ```agda
   data DType : Type where
     float32 int64 bool : DType

   record Tensor (dt : DType) : Type where
     ...
   ```

3. **Device specification**: CPU vs GPU
   ```agda
   data Device : Type where
     cpu gpu : Device

   eval-pytorch-on : Device â†’ String â†’ List Tensor â†’ Tensor
   ```

4. **Gradients**: Extend oracle to return gradients
   ```agda
   eval-pytorch-grad : String â†’ List Tensor â†’ (Tensor Ã— List Tensor)
   ```

5. **Numerical properties**: Postulate axioms about precision
   ```agda
   postulate
     eval-associative : (e1 e2 e3 : Einsum _ _)
                      â†’ eval (e1 â¨¾ (e2 â¨¾ e3)) â‰ˆ eval ((e1 â¨¾ e2) â¨¾ e3)
   ```
   where `â‰ˆ` is approximate equality up to floating point error

6. **Performance model**: Cost estimation
   ```agda
   cost : Einsum ins out â†’ Nat  -- FLOP count
   ```
-}
