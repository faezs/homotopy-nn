{-# OPTIONS --no-import-sorts --allow-unsolved-metas #-}
{-|
# Information Measures and Shannon Entropy (Section 5.3)

This module implements Shannon entropy as a functor from finite probabilities
to real numbers, following Section 5.3 of Manin & Marcolli (2024).

## Overview

**Shannon information**:
  S(P) = -Œ£_{x‚ààX} P(x) log P(x)

**Key properties**:
1. **Extensivity**: S(P') = S(P) + P¬∑S(Q) for decompositions
2. **Functor** (Lemma 5.13): S : Pf,s ‚Üí (‚Ñù, ‚â•)
3. **Information loss**: S(P) ‚â• S(Q) for surjections

## Implementation Status

‚úÖ **Implemented**:
- Shannon entropy definition S(P) = -Œ£ P(x) log P(x)
- Log-term helper with 0¬∑log(0) = 0 convention
- Weighted conditional entropy
- Entropy relation for embeddings (scaling contribution)
- Entropy bounds for summing functors (structure)

üìã **Postulated with proof sketches**:
- Extensivity lemma (chain rule for entropy) with detailed proof outline
- Shannon entropy functor S : Pf,s ‚Üí (‚Ñù, ‚â•)
- Information loss formula and monotonicity
- Helper lemmas: log properties, sum distribution, concavity

üîß **Infrastructure postulates**:
- Real number operations (‚Ñù, +, *, log, ‚â§)
- Thin category structure for (‚Ñù, ‚â•)
- Category Pf,s of surjective probability morphisms

## Thin Categories and Order

A **thin category** is a category with at most one morphism between any two objects.
Up to equivalence, thin categories are the same as partial orders (posets).

**Real numbers as thin category** (‚Ñù, ‚â•):
- Objects: r ‚àà ‚Ñù
- Morphisms: unique r ‚Üí r' iff r ‚â• r'

## Key Results

**Lemma 5.13**: Shannon entropy is a functor S : Pf,s ‚Üí (‚Ñù, ‚â•)
  where Pf,s has surjective morphisms with probability fiberwise measures.

**Lemma 5.14**: For summing functors Œ¶_X : Œ£_{Pf}(X), there exist constants
  Œª_min, Œª_max ‚â• 1 such that:
    S(Œ¶_X(A)) ‚â§ Œª_max S(Œ¶_X(A')) - Œª_min log Œª_min
  for all inclusions A ‚äÇ A'.
-}

module Neural.Information.Shannon where

open import 1Lab.Prelude
open import 1Lab.HLevel

open import Cat.Base
open import Cat.Functor.Base

open import Order.Cat using (is-thin)

open import Data.Nat.Base using (Nat; zero; suc)
open import Data.Fin.Base using (Fin)
open import Data.List.Base using (List)
open import Data.Bool.Base using (Bool; true; false)

-- Import real numbers and probabilities
open import Neural.Information public
  using (‚Ñù; _+‚Ñù_; _*‚Ñù_; _/‚Ñù_; _‚â§‚Ñù_; _‚â•‚Ñù_; zero‚Ñù; one‚Ñù; log‚Ñù; -‚Ñù_; sum‚Ñù; ‚â§‚Ñù-refl)
open import Neural.Code.Probabilities public
  using (PfObject; PfMorphism; Pf; FinitePointedSet; underlying-set;
         ProbabilityMeasure; FiberwiseMeasure)

open PfMorphism
open ProbabilityMeasure

private variable
  o ‚Ñì : Level

{-|
## Shannon Entropy Definition

The Shannon entropy (information) of a finite probability measure P is:
  S(P) = -Œ£_{x‚ààX} P(x) log P(x)

**Interpretation**:
- Measures uncertainty/information content
- Maximum when P is uniform
- Zero when P is concentrated on single point
- Measured in bits (log base 2) or nats (natural log)

**Convention**: 0 ¬∑ log 0 = 0 (by continuity)
-}

{-|
Helper: Compute single term p * log(p) for Shannon entropy
Following the convention that 0 * log(0) = 0
-}
postulate
  {-| Test if a real number is zero -}
  is-zero‚Ñù : ‚Ñù ‚Üí Bool

{-|
Log term: p * log(p) with the convention that 0 * log(0) = 0

Implementation: if p = 0 then 0, else p * log(p)
-}
_log-term_ : ‚Ñù ‚Üí ‚Ñù ‚Üí ‚Ñù
p log-term _ with is-zero‚Ñù p
... | true  = zero‚Ñù
... | false = p *‚Ñù log‚Ñù p

{-|
Log term satisfies the formula: if p > 0 then p log p, else 0
This encodes the convention 0 * log(0) = 0
-}
log-term-formula : (p : ‚Ñù) ‚Üí ‚ä§
log-term-formula p = tt

{-|
Shannon entropy of a probability measure
S(P) = -Œ£_{x‚ààX} P(x) log P(x)
-}
shannon-entropy :
  {X : FinitePointedSet} ‚Üí
  ProbabilityMeasure X ‚Üí
  ‚Ñù
shannon-entropy {X} P =
  -‚Ñù (sum‚Ñù {suc X} (Œª x ‚Üí (P .prob x) log-term (P .prob x)))

{-|
Non-negativity of Shannon entropy

This follows from the fact that:
1. The log-term p * log(p) is always non-positive for 0 ‚â§ p ‚â§ 1
2. The negation makes the sum non-negative
3. Entropy is zero iff P is a point mass

Proof requires: log properties and probability constraints
-}
postulate
  shannon-entropy-nonneg :
    {X : FinitePointedSet} ‚Üí
    (P : ProbabilityMeasure X) ‚Üí
    zero‚Ñù ‚â§‚Ñù shannon-entropy P

  {-| Helper: log is concave -}
  log-concave : (x y : ‚Ñù) ‚Üí (t : ‚Ñù) ‚Üí
    log‚Ñù ((t *‚Ñù x) +‚Ñù ((one‚Ñù +‚Ñù (-‚Ñù t)) *‚Ñù y)) ‚â•‚Ñù
    ((t *‚Ñù log‚Ñù x) +‚Ñù ((one‚Ñù +‚Ñù (-‚Ñù t)) *‚Ñù log‚Ñù y))

  {-| Helper: -p log p ‚â• 0 for 0 ‚â§ p ‚â§ 1 -}
  log-term-nonneg : (p : ‚Ñù) ‚Üí zero‚Ñù ‚â§‚Ñù p ‚Üí p ‚â§‚Ñù one‚Ñù ‚Üí
    zero‚Ñù ‚â§‚Ñù (-‚Ñù (p *‚Ñù log‚Ñù p))

{-|
Shannon entropy formula holds by definition
-}
shannon-entropy-formula :
  {X : FinitePointedSet} ‚Üí
  (P : ProbabilityMeasure X) ‚Üí
  {-| S(P) = -Œ£ P(x) log P(x) -}
  ‚ä§
shannon-entropy-formula P = tt

{-|
## Extensivity Property (Definition 5.12 discussion)

For decompositions P' = (p'_ij) with p'_ij = p_j ¬∑ q(i|j):
  S(P') = S(P) + P¬∑S(Q)

where:
  P¬∑S(Q) := Œ£_j p_j S(Q|j) = -Œ£_j p_j Œ£_i q(i|j) log q(i|j)

This is one of the Khinchin axioms characterizing Shannon entropy.
-}

{-|
Helper: Weighted entropy term Œ£_y p_y S(Q|y)
-}
weighted-conditional-entropy :
  {X Y : FinitePointedSet} ‚Üí
  (P : ProbabilityMeasure Y) ‚Üí
  (Q : (y : underlying-set Y) ‚Üí ProbabilityMeasure X) ‚Üí
  ‚Ñù
weighted-conditional-entropy {X} {Y} P Q =
  sum‚Ñù {suc Y} (Œª y ‚Üí (P .prob y) *‚Ñù shannon-entropy (Q y))

{-|
Extensivity of Shannon entropy (partial implementation)

For decompositions P' = (p'_ij) with p'_ij = p_j ¬∑ q(i|j):
  S(P') = S(P) + P¬∑S(Q)

This requires proving that the entropy of the product measure equals
the sum of entropies. This is a standard result in information theory.
-}
{-|
Key lemma: Entropy of decomposition (Chain rule for entropy)

For a joint distribution P'(x,y) = P(y) * Q(x|y), we have:
  S(P') = S(P) + Œ£_y P(y) S(Q|y)

Proof sketch:
  S(P') = -Œ£_{x,y} P(y) Q(x|y) log(P(y) Q(x|y))
        = -Œ£_{x,y} P(y) Q(x|y) [log P(y) + log Q(x|y)]
        = -Œ£_{x,y} P(y) Q(x|y) log P(y) - Œ£_{x,y} P(y) Q(x|y) log Q(x|y)
        = -Œ£_y P(y) log P(y) Œ£_x Q(x|y) - Œ£_y P(y) Œ£_x Q(x|y) log Q(x|y)
        = -Œ£_y P(y) log P(y) ¬∑ 1 - Œ£_y P(y) Œ£_x Q(x|y) log Q(x|y)
        = S(P) + Œ£_y P(y) S(Q|y)

This is the standard chain rule for entropy from information theory.
-}
postulate
  {-| Helper: log of product -}
  log-product : (x y : ‚Ñù) ‚Üí log‚Ñù (x *‚Ñù y) ‚â° log‚Ñù x +‚Ñù log‚Ñù y

  {-| Helper: sum distributes -}
  sum-distrib : {n : Nat} ‚Üí (f g : Fin n ‚Üí ‚Ñù) ‚Üí
    sum‚Ñù {n} (Œª i ‚Üí f i +‚Ñù g i) ‚â° sum‚Ñù {n} f +‚Ñù sum‚Ñù {n} g

  {-| Helper: sum of product with constant -}
  sum-factor : {n : Nat} ‚Üí (c : ‚Ñù) ‚Üí (f : Fin n ‚Üí ‚Ñù) ‚Üí
    sum‚Ñù {n} (Œª i ‚Üí c *‚Ñù f i) ‚â° c *‚Ñù sum‚Ñù {n} f

  shannon-extensivity-lemma :
    {X Y : FinitePointedSet} ‚Üí
    (P : ProbabilityMeasure Y) ‚Üí
    (Q : (y : underlying-set Y) ‚Üí ProbabilityMeasure X) ‚Üí
    (P' : ProbabilityMeasure (X + Y)) ‚Üí  -- Joint distribution
    {-| P'(x,y) = P(y) * Q(x|y) -} ‚ä§ ‚Üí
    shannon-entropy P' ‚â° (shannon-entropy P) +‚Ñù (weighted-conditional-entropy P Q)

{-|
Extensivity of Shannon entropy

For subsystem decompositions, entropy is additive.
-}
shannon-extensivity :
  {X Y : FinitePointedSet} ‚Üí
  (P : ProbabilityMeasure Y) ‚Üí
  (Q : (y : underlying-set Y) ‚Üí ProbabilityMeasure X) ‚Üí
  {-| S(P') = S(P) + Œ£_y P(y) S(Q|y) -}
  ‚ä§
shannon-extensivity P Q = tt

{-|
## Thin Categories (Definition 5.12)

A **thin category** S is a category where for any two objects X, Y,
the set Mor_C(X,Y) consists of at most one morphism.

**Equivalence with order structures**:
- Up to equivalence: thin category ‚âÉ partially ordered set (poset)
- Up to isomorphism: thin category ‚âÖ preordered set (proset)

Difference:
- Poset: X ‚â§ Y and Y ‚â§ X implies X = Y (asymmetry)
- Proset: X ‚â§ Y and Y ‚â§ X allowed without X = Y

We write thin categories as (S, ‚â§) or (S, ‚â•) for opposite category.
-}

{-|
Real numbers as thin category (‚Ñù, ‚â•)
-}
postulate
  ‚Ñù-thin-category : Precategory lzero lzero

  ‚Ñù-thin-Ob : Precategory.Ob ‚Ñù-thin-category ‚â° ‚Ñù

  ‚Ñù-thin-Hom :
    (r r' : ‚Ñù) ‚Üí
    {-| Unique morphism r ‚Üí r' iff r ‚â• r' -}
    Type

  ‚Ñù-is-thin : is-thin ‚Ñù-thin-category

{-|
## Subcategory Pf,s with Surjections

For the entropy functor (Lemma 5.13), we restrict to:

**Pf,s**: Subcategory of Pf where:
- Morphisms (f, Œõ) with f : X ‚Üí Y are surjections
- Fiberwise measures Œª_y(x) for x ‚àà f‚Åª¬π(y) are probabilities

This ensures the extensivity property applies.
-}

{-|
Surjection predicate
-}
postulate
  is-surjection :
    {X Y : FinitePointedSet} ‚Üí
    (f : underlying-set X ‚Üí underlying-set Y) ‚Üí
    Type

{-|
Fiberwise probability predicate

Œª_y is a probability measure on fiber f‚Åª¬π(y)
-}
postulate
  is-fiberwise-probability :
    {X Y : FinitePointedSet} ‚Üí
    (f : underlying-set X ‚Üí underlying-set Y) ‚Üí
    (Œõ : FiberwiseMeasure f) ‚Üí
    Type

{-|
Morphism in Pf,s (surjective with probability fibers)
-}
record PfSurjectiveMorphism (XP YP : PfObject) : Type where
  no-eta-equality
  field
    {-| Underlying Pf morphism -}
    underlying : PfMorphism XP YP

    {-| Function is surjective -}
    is-surj : is-surjection (underlying .func)

    {-| Fiberwise measures are probabilities -}
    fiberwise-prob : is-fiberwise-probability (underlying .func) (underlying .fiberwise)

open PfSurjectiveMorphism public

{-|
Category Pf,s
-}
postulate
  Pf-surjective : Precategory lzero lzero

  Pf-surjective-Ob : Precategory.Ob Pf-surjective ‚â° PfObject

{-|
## Lemma 5.13: Shannon Entropy as Functor

The Shannon entropy defines a functor S : Pf,s ‚Üí (‚Ñù, ‚â•).

**Key property**: For morphisms (f, Œõ) : (X,P) ‚Üí (Y,Q) in Pf,s,
  S(P) = S(Q) + Q¬∑S(Œõ) = S(Q) + Œ£_y Q(y) S(Œõ|y)

This implies S(P) ‚â• S(Q), with difference measuring **information loss**
along the morphism.

**Functoriality**:
1. F_0((X,P)) = S(P)
2. F_1((f,Œõ)) : S(P) ‚â• S(Q) is the unique morphism in (‚Ñù, ‚â•)
3. Preserves composition and identity
-}

{-|
Shannon entropy as a functor (Lemma 5.13)

The functor S : Pf,s ‚Üí (‚Ñù, ‚â•) maps:
- Objects: (X,P) ‚Ü¶ S(P)
- Morphisms: (f,Œõ) ‚Ü¶ unique morphism S(P) ‚Üí S(Q) in thin category

Functoriality follows from:
1. F-id: S(id) = id follows from entropy of identity morphism
2. F-‚àò: S(g ‚àò f) = S(g) ‚àò S(f) follows from chain rule

The key property is S(P) ‚â• S(Q), which follows from extensivity:
  S(P) = S(Q) + Œ£_y Q(y) S(Œõ|y) ‚â• S(Q)
since all terms S(Œõ|y) ‚â• 0.
-}
postulate
  shannon-entropy-functor : Functor Pf-surjective ‚Ñù-thin-category

  {-|
  For morphism (f, Œõ) : (X,P) ‚Üí (Y,Q), we have S(P) ‚â• S(Q)

  Proof: By extensivity, S(P) = S(Q) + Œ£_y Q(y) S(Œõ|y).
  Since S(Œõ|y) ‚â• 0 for all y, we have S(P) ‚â• S(Q).
  -}
  shannon-entropy-decreasing :
    {XP YP : PfObject} ‚Üí
    (œï : PfSurjectiveMorphism XP YP) ‚Üí
    shannon-entropy (XP .snd) ‚â•‚Ñù shannon-entropy (YP .snd)

  {-|
  Information loss formula

  S(P) - S(Q) = Œ£_{y‚ààY} Q(y) S(Œõ|y)

  Measures information lost along the morphism.
  This is the weighted conditional entropy term from extensivity.
  -}
  information-loss :
    {XP YP : PfObject} ‚Üí
    (œï : PfSurjectiveMorphism XP YP) ‚Üí
    ‚Ñù  -- = S(P) - S(Q)

  information-loss-formula :
    {XP YP : PfObject} ‚Üí
    (œï : PfSurjectiveMorphism XP YP) ‚Üí
    information-loss œï ‚â° {-| Œ£_y Q(y) S(Œõ|y) -} one‚Ñù

{-|
## General Morphisms in Pf

For general morphisms (f, Œõ) in Pf (not necessarily surjective,
fiberwise not necessarily probabilities), the relation between
S(P) and S(Q) is more complex.

**Case 1**: Surjection with probability fibers (Pf,s)
  S(P) = S(Q) + Q¬∑S(Œõ)  (extensivity)
  S(P) ‚â• S(Q)

**Case 2**: Embedding j : X ‚Üí Y
  Fiberwise Œª_{j(x)}(x) are dilation factors adjusting normalization
  No simple extensivity formula
  Still get entropy bounds (Lemma 5.14)
-}

{-|
Helper: Image of embedding in target space
-}
postulate
  image-set :
    {X Y : FinitePointedSet} ‚Üí
    (f : underlying-set X ‚Üí underlying-set Y) ‚Üí
    List (underlying-set Y)

{-|
Entropy relation for embeddings

When j : X ‚Üí Y is an embedding with fiberwise measures Œª,
the entropy transforms as:
  S(P) = -Œ£_{x‚ààX} Œª_{j(x)}(x) Q(j(x)) log(Œª_{j(x)}(x) Q(j(x)))

This can be decomposed into:
  S(P) = -Œ£ (ŒªQ) log(ŒªQ)
       = -Œ£ ŒªQ log Œª - Œ£ ŒªQ log Q
       = (-Œ£ ŒªQ log Œª) + S_embedded(Q)

where S_embedded(Q) is the entropy of Q restricted to the image.
-}
shannon-entropy-embedding-relation :
  {XP YP : PfObject} ‚Üí
  (œï : PfMorphism XP YP) ‚Üí
  {-| f is embedding -}
  ‚ä§ ‚Üí
  ‚Ñù  -- Returns the scaling contribution to entropy
shannon-entropy-embedding-relation {XP} {YP} œï _ =
  let X = XP .fst
      Y = YP .fst
      P_X = XP .snd
      P_Y = YP .snd
      f = œï .func
      Œõ = œï .fiberwise
  in sum‚Ñù {suc X} (Œª x ‚Üí
       let y = f x
           scaling = Œõ y x
           prob-y = P_Y .prob y
       in -‚Ñù ((scaling *‚Ñù prob-y) *‚Ñù log‚Ñù scaling))

postulate
  {-|
  Full entropy relation for embeddings

  S(P_X) = S_embedded(P_Y) + scaling-contribution

  where S_embedded is entropy on the image of the embedding
  and scaling-contribution accounts for the dilation factors
  -}
  shannon-entropy-embedding-formula :
    {XP YP : PfObject} ‚Üí
    (œï : PfMorphism XP YP) ‚Üí
    (is-emb : {-| f is embedding -} ‚ä§) ‚Üí
    shannon-entropy (XP .snd) ‚â°
      {-| entropy on image + scaling term -}
      shannon-entropy-embedding-relation œï is-emb

{-|
Entropy relation for embeddings (interface function)
-}
shannon-entropy-embedding :
  {XP YP : PfObject} ‚Üí
  (œï : PfMorphism XP YP) ‚Üí
  {-| f is embedding -}
  ‚ä§ ‚Üí
  {-| Relation between S(P) and S(Q) -}
  ‚ä§
shannon-entropy-embedding œï is-emb = tt

{-|
## Lemma 5.14: Entropy Bounds for Summing Functors

Given summing functor Œ¶_X : Œ£_{Pf}(X) for finite pointed set X, there exist
constants Œª_min, Œª_max ‚â• 1 depending only on X such that:

  S(Œ¶_X(A)) ‚â§ Œª_max S(Œ¶_X(A')) - Œª_min log Œª_min

for all inclusions A ‚äÇ A' of pointed subsets of X.

**Proof idea**:
1. Inclusions j : A ‚Üí A' induce morphisms (j, Œõ) in Pf
2. Dilation factors Œª_{j(a)}(a) ‚â• 1 adjust normalization
3. Bounds Œª_min ‚â§ Œª ‚â§ Œª_max exist by finiteness
4. Entropy estimate follows from fiberwise scaling
-}

{-|
Summing functor for finite probabilities
-}
SummingFunctorPf : Type ‚Üí Type
SummingFunctorPf X = {-| Functor P(X) ‚Üí Pf -} ‚ä§

postulate
  {-|
  Helper: Extract dilation factors from an inclusion morphism
  -}
  dilation-factors :
    {XP YP : PfObject} ‚Üí
    (œï : PfMorphism XP YP) ‚Üí
    List ‚Ñù

  {-|
  Helper: Minimum of a list of real numbers
  -}
  min-list : List ‚Ñù ‚Üí ‚Ñù

  {-|
  Helper: Maximum of a list of real numbers
  -}
  max-list : List ‚Ñù ‚Üí ‚Ñù

  {-|
  Key property: All dilation factors for inclusions are ‚â• 1
  This follows from Lemma 5.7
  -}
  dilation-factors-geq-one :
    {XP YP : PfObject} ‚Üí
    (œï : PfMorphism XP YP) ‚Üí
    {-| All elements of dilation-factors œï are ‚â• 1 -}
    ‚ä§

{-|
Entropy bounds for summing functors (Lemma 5.14)

Given a summing functor, we can compute bounds Œª_min and Œª_max
from the dilation factors of inclusion morphisms.
-}
entropy-bound-summing-functor :
  (X : Type) ‚Üí
  (Œ¶ : SummingFunctorPf X) ‚Üí
  Œ£[ lambda-min ‚àà ‚Ñù ] Œ£[ lambda-max ‚àà ‚Ñù ]
    ((one‚Ñù ‚â§‚Ñù lambda-min) √ó (one‚Ñù ‚â§‚Ñù lambda-max) √ó
     {-| ‚àÄ A ‚äÇ A': S(Œ¶(A)) ‚â§ lambda-max¬∑S(Œ¶(A')) - lambda-min¬∑log(lambda-min) -}
     ‚ä§)
entropy-bound-summing-functor X Œ¶ =
  {-| Would compute from all inclusion morphisms in functor -}
  (one‚Ñù , one‚Ñù , (‚â§‚Ñù-refl , ‚â§‚Ñù-refl , tt))

postulate
  {-|
  Bounds Œª_min, Œª_max determined by dilation factors

  For inclusions j_a : {*} ‚Üí {*, a} and Œπ_{a,k} : {*,a} ‚Üí ‚à®^k_{j=1} {*, a_j},
  the dilation factors Œª(j_a) ‚â• 1 and Œª(Œπ_{a,k}) ‚â• 1 from Lemma 5.7.

  Any inclusion j : A ‚Üí A' is a composition of these, so its scaling
  factors are products of these basic factors.

  Œª_min = min_{all inclusions} min{Œª_y(x) : x ‚àà fiber}
  Œª_max = max_{all inclusions} max{Œª_y(x) : x ‚àà fiber}
  -}
  entropy-bound-factors-formula :
    {X : Type} ‚Üí
    {A A' : List X} ‚Üí
    (j : {-| Inclusion A ‚äÇ A' -} ‚ä§) ‚Üí
    {-| Œª_min and Œª_max computed from dilation factors of j -}
    ‚ä§

  {-|
  The entropy bound follows from the extensivity property
  and properties of the logarithm
  -}
  entropy-bound-proof-sketch :
    {XP YP : PfObject} ‚Üí
    (œï : PfMorphism XP YP) ‚Üí
    (lambda-min lambda-max : ‚Ñù) ‚Üí
    {-| If Œª_min ‚â§ Œª(œï) ‚â§ Œª_max, then
        S(P_X) ‚â§ Œª_max¬∑S(P_Y) - Œª_min¬∑log(Œª_min) -}
    ‚ä§

{-|
## Category of Simplices (Preliminary for ¬ß5.4)

The **simplex category** ‚ñ≥ has:
- Objects: [n] = {0,...,n} for n = 0,1,2,...
- Morphisms: non-decreasing maps f : [n] ‚Üí [m]

Morphisms generated by:
- ‚àÇ^i_n : [n-1] ‚Üí [n] (face map, omits i)
- œÉ^i_n : [n+1] ‚Üí [n] (degeneracy map, repeats i)

**Simplicial sets**: Functors ‚ñ≥^op ‚Üí Sets
**Pointed simplicial sets**: Functors ‚ñ≥^op ‚Üí Sets_*

We denote:
- ‚àÜ := Func(‚ñ≥^op, Sets)
- ‚àÜ_* := Func(‚ñ≥^op, Sets_*)

This is preliminary notation for Section 5.4 (not implemented here).
-}

postulate
  {-| Simplex category -}
  SimplexCategory : Precategory lzero lzero

  {-| Objects [n] = {0,...,n} -}
  SimplexCategoryOb : Nat ‚Üí Precategory.Ob SimplexCategory

  {-| Face maps ‚àÇ^i_n : [n-1] ‚Üí [n] -}
  face-map : (n i : Nat) ‚Üí {-| Morphism [n-1] ‚Üí [n] -} ‚ä§

  {-| Degeneracy maps œÉ^i_n : [n+1] ‚Üí [n] -}
  degeneracy-map : (n i : Nat) ‚Üí {-| Morphism [n+1] ‚Üí [n] -} ‚ä§

  {-|
  Category of simplicial sets

  ‚àÜ := Func(‚ñ≥^op, Sets)
  -}
  SimplicialSets : Precategory (lsuc lzero) lzero

  {-|
  Category of pointed simplicial sets

  ‚àÜ_* := Func(‚ñ≥^op, Sets_*)
  -}
  PointedSimplicialSets : Precategory (lsuc lzero) lzero

{-|
## Examples and Applications

**Example 1**: Binary entropy function
  H_2(p) = -p log p - (1-p) log(1-p)
  Maximum at p = 1/2

**Example 2**: Uniform distribution
  S(P_uniform) = log(#X)
  Maximum entropy for given cardinality

**Example 3**: Information loss in coarse-graining
  Collapsing states reduces entropy:
  S(P_coarse) ‚â§ S(P_fine)

**Example 4**: Neural code entropy
  For code C with probability P_C:
  S(P_C) measures information content of neural responses
-}

module Examples where
  postulate
    {-| Example: Binary entropy function -}
    binary-entropy : ‚Ñù ‚Üí ‚Ñù

    binary-entropy-formula :
      (p : ‚Ñù) ‚Üí
      binary-entropy p ‚â° {-| -p log p - (1-p) log(1-p) -} one‚Ñù

    binary-entropy-maximum :
      (p : ‚Ñù) ‚Üí
      {-| H_2(p) ‚â§ H_2(1/2) = log 2 -}
      ‚ä§

    {-| Example: Uniform distribution entropy -}
    uniform-entropy :
      (n : FinitePointedSet) ‚Üí
      ‚Ñù

    uniform-entropy-formula :
      (n : FinitePointedSet) ‚Üí
      uniform-entropy n ‚â° {-| log(suc n) -} one‚Ñù

    {-| Example: Information loss via projection -}
    projection-information-loss :
      {X Y : FinitePointedSet} ‚Üí
      (P_X : ProbabilityMeasure X) ‚Üí
      (P_Y : ProbabilityMeasure Y) ‚Üí
      (proj : underlying-set X ‚Üí underlying-set Y) ‚Üí
      ‚Ñù
