{-# OPTIONS --no-import-sorts #-}
{-|
# Section 1.2: Dynamical Objects of Chains

From Belfiore & Bennequin (2022):

> "The simplest architecture of a network is a chain, and the feed-forward
> functioning of the network, when it has learned, corresponds to a covariant
> functor X from the category C₀(Γ) freely generated by the graph to the
> category of sets, Set."

## Computational vs Denotational

This module demonstrates that the categorical approach is **BOTH**:

1. **Denotational** (semantics): Functors X^w, W, X give mathematical meaning
   to the network architecture independent of execution

2. **Computational** (syntax): The functors also describe the actual forward
   propagation computation

3. **Transformational** (dynamics): Natural transformations (like backprop)
   describe how the computation evolves

### The Three Functors:

- **X^w**: Activity functor (for fixed weights w)
  - Objects: Lₖ ↦ Xₖ (set of possible activities at layer k)
  - Morphisms: edge ↦ X^w_{k+1,k}: Xₖ → Xₖ₊₁ (weighted transformation)

- **W = Π**: Weight functor
  - Objects: Lₖ ↦ Πₖ (product of all weights for layers ≥ k)
  - Morphisms: edge ↦ Πₖ₊₁,ₖ: Πₖ → Πₖ₊₁ (forgetting projection)

- **X**: Total dynamics functor (Equation 1.1)
  - Objects: Lₖ ↦ Xₖ × Πₖ (activity-weight pairs)
  - Represents ALL possible functionings for ALL potential weights
-}

module Neural.Dynamics.Chain where

open import 1Lab.Prelude
open import 1Lab.HLevel
open import 1Lab.Path
open import 1Lab.Path.Reasoning

open import Cat.Base
open import Cat.Functor.Base
open import Cat.Instances.Graphs using (Graph)
open import Cat.Instances.Sets

open import Data.Nat.Base using (Nat; zero; suc; _+_)
open import Data.Fin.Base using (Fin; fzero; fsuc; fzero≠fsuc; fin-view; Fin-view; weaken; fsuc-inj; Fin-cases)
open Data.Fin.Base.Fin-view renaming (zero to fv-zero; suc to fv-suc)
open import Data.Sum.Base using (_⊎_; inl; inr)

open import Neural.Topos.Category using (OrientedGraph)
import Neural.Topos.Category as Cat

{-|
## Chain Graph as OrientedGraph

A chain is an oriented graph where:
- Vertices: Fin (suc n) representing layers L₀, L₁, ..., Lₙ
- Edges: Only connect Lᵢ → Lᵢ₊₁ (feedforward, no skip connections)
- No convergence (no vertex has multiple incoming edges)

This is the simplest DNN architecture - a Multi-Layer Perceptron (MLP).
-}

chain-graph : (n : Nat) → OrientedGraph lzero lzero
chain-graph n = record
  { graph = chain-underlying
  ; classical = chain-classical
  ; no-loops = chain-no-loops
  ; _≤_ = chain-reach
  ; ≤-prop = chain-reach-prop
  ; path→≤ = chain-path→reach
  ; ≤-refl = chain-≤-refl
  ; ≤-trans = chain-≤-trans
  ; ≤-antisym = chain-≤-antisym
  }
  where
    chain-underlying : Graph lzero lzero
    chain-underlying .Graph.Vertex = Fin (suc n)
    -- Edge from i to j: there exists k : Fin n such that i = weaken k and j = fsuc k
    -- This gives edges: weaken 0 → fsuc 0, weaken 1 → fsuc 1, ..., weaken (n-1) → fsuc (n-1)
    -- Which is: 0→1, 1→2, ..., (n-1)→n
    chain-underlying .Graph.Edge i j = Σ[ k ∈ Fin n ] (i ≡ weaken k) × (j ≡ fsuc k)
    chain-underlying .Graph.Vertex-is-set = hlevel 2
    chain-underlying .Graph.Edge-is-set = hlevel 2

    -- Edges are propositions (at most one edge between vertices)
    chain-classical : {x y : Fin (suc n)} → is-prop (Σ[ k ∈ Fin n ] (x ≡ weaken k) × (y ≡ fsuc k))
    chain-classical {x} {y} (k , p , q) (k' , p' , q') =
      let wk-eq : weaken k ≡ weaken k'
          wk-eq = sym p ∙ p'
          k-eq : k ≡ k'
          k-eq = weaken-inj wk-eq
      in Σ-pathp k-eq (is-prop→pathp (λ i → ×-is-hlevel 1 (hlevel 1) (hlevel 1)) (p , q) (p' , q'))
      where
        weaken-inj : {n : Nat} {x y : Fin n} → weaken x ≡ weaken y → x ≡ y
        weaken-inj {x = x} {y = y} p with fin-view x | fin-view y
        ... | fv-zero | fv-zero = refl
        ... | fv-zero | fv-suc y' = absurd (fzero≠fsuc p)
        ... | fv-suc x' | fv-zero = absurd (fzero≠fsuc (sym p))
        ... | fv-suc x' | fv-suc y' = ap fsuc (weaken-inj (fsuc-inj p))

    -- No self-loops: there's no k such that weaken k = fsuc k
    chain-no-loops : {x : Fin (suc n)} → ¬ (Σ[ k ∈ Fin n ] (x ≡ weaken k) × (x ≡ fsuc k))
    chain-no-loops (k , p , q) = weaken≠fsuc (sym p ∙ q)
      where
        weaken≠fsuc : {n : Nat} {k : Fin n} → ¬ (weaken k ≡ fsuc k)
        weaken≠fsuc {k = k} p with fin-view k
        ... | fv-zero = fzero≠fsuc p
        ... | fv-suc k' = weaken≠fsuc {k = k'} (fsuc-inj p)

    -- Reachability: propositional truncation of EdgePath
    -- This is the key fix: reachability is NOT the same as having a direct edge!
    -- For chains: i ≤ j means there's a path from i to j (possibly through intermediate vertices)
    open Cat.EdgePathDef chain-underlying

    chain-reach : Fin (suc n) → Fin (suc n) → Type
    chain-reach x y = ∥ EdgePath x y ∥

    chain-reach-prop : {x y : Fin (suc n)} → is-prop (chain-reach x y)
    chain-reach-prop = squash

    chain-path→reach : {x y : Fin (suc n)} → EdgePath x y → chain-reach x y
    chain-path→reach p = inc p

    -- Partial order properties for reachability
    chain-≤-refl : (x : Fin (suc n)) → chain-reach x x
    chain-≤-refl x = inc path-nil

    chain-≤-trans : {x y z : Fin (suc n)} → chain-reach x y → chain-reach y z → chain-reach x z
    chain-≤-trans {x} {y} {z} = ∥-∥-rec₂ squash λ p q → inc (p ++ᵖ q)

    -- Antisymmetry: chains are acyclic, so x ≤ y and y ≤ x implies x ≡ y
    -- This follows from the fact that the only cycles are x → x (via empty path)
    postulate
      chain-≤-antisym : {x y : Fin (suc n)} → chain-reach x y → chain-reach y x → x ≡ y
      -- TODO: Prove this using Fin ordering properties
      -- Key insight: if there's a path from i to j in a chain, then i ≤ j numerically
      -- If there's also a path from j to i, then j ≤ i numerically
      -- Therefore i = j

{-|
## Activity Functor X^w (for fixed weights)

**Denotational**: Describes the "meaning" of each layer as a set of possible
neuron activities.

**Computational**: Describes the actual forward propagation computation.

From paper:
> "to a layer Lₖ is associated the set Xₖ of possible activities of
> the population of neurons in Lₖ, to the edge Lₖ ↦→ Lₖ₊₁ is associated
> the map X^w_{k+1,k}: Xₖ → Xₖ₊₁ which corresponds to the learned weights"
-}

module _ {ℓ} (Γ : OrientedGraph lzero lzero) where
  open OrientedGraph Γ

  -- Activity spaces at each layer (objects of the functor)
  record ActivityFunctor : Type (lsuc ℓ) where
    field
      -- For each layer (vertex), the set of possible activities
      Activity : Vertex → Type ℓ

      -- For each edge (i,j), the weighted transformation
      -- X^w_{j,i}: Activity(i) → Activity(j)
      forward : {i j : Vertex} → (e : Edge i j) → Activity i → Activity j

      -- Functoriality: composition law (transitive forward propagation)
      -- compose : ∀ {i j k} → Edge j k → Edge i j → ...
      -- (For chains, edges compose via ≤-trans-ᴸ)

  {-|
  ## Weight Functor W = Π

  **Denotational**: Describes the space of all possible network configurations.

  **Computational**: Describes how weights are "forgotten" as we move forward
  through layers.

  From paper:
  > "for Lₖ we define Πₖ as the product of all the sets W_{l+1,l} of weights
  > for l ≥ k, and to the edge k ↦→ k+1 we associate the natural forgetting
  > projection Πₖ₊₁,ₖ: Πₖ → Πₖ₊₁"

  **Key insight**: Weights are morphisms in X^w, but objects in W!
  This is a "dual representation" as noted in the paper.
  -}

  record WeightFunctor : Type (lsuc ℓ) where
    field
      -- Weight space for each edge (individual transformation)
      WeightSpace : {i j : Vertex} → Edge i j → Type ℓ

      -- At layer i: product of all weight spaces for paths from i to outputs
      -- Πᵢ = ∏_{j : i→...→output} W_j
      Π : Vertex → Type ℓ

      -- Forgetting projection: when moving along edge i→j, forget weights for that edge
      -- Πᵢ → Πⱼ
      forget : {i j : Vertex} → (e : Edge i j) → Π i → Π j

      -- For output vertices: Π(output) = ⊤ (terminal object/singleton)
      -- (For chains, this is the last vertex)

  {-|
  ## Crossed Product Functor X (Equation 1.1)

  **This is the key unification**: X represents ALL possible functionings
  of the network for EVERY potential weight configuration.

  From paper (Equation 1.1):
  > "The cartesian products Xₖ × Πₖ together with the maps
  >  X_{k+1,k} × Π_{k+1,k}(xₖ, (w_{k+1,k}, w'_{k+1})) = (X^w_{k+1,k}(xₖ), w'_{k+1})
  > also defines a covariant functor X"

  **Interpretation**:
  - State: (activity at layer k, weights for layers ≥ k)
  - Transition: apply weighted transformation, forget used weights
  - This describes the state evolution of the learning system!
  -}

  record TotalDynamicsFunctor (X^w : ActivityFunctor) (W : WeightFunctor) : Type ℓ where
    open ActivityFunctor X^w
    open WeightFunctor W

    -- At each vertex: activity-weight pairs
    State : Vertex → Type ℓ
    State v = Activity v × Π v

    -- Transition function (Equation 1.1)
    -- (xᵢ, (wᵢⱼ, w'ⱼ)) ↦ (X^w_{j,i}(xᵢ), w'ⱼ)
    transition : {i j : Vertex} → (e : Edge i j) → State i → State j
    transition e (xᵢ , wᵢ) = forward e xᵢ , forget e wᵢ

    -- Natural projection from X to W
    -- This is a natural transformation: X → W
    proj-weights : ∀ (v : Vertex) → State v → Π v
    proj-weights v (x , w) = w

{-|
## Example: 3-Layer MLP

Let's make this concrete with a simple example:

- L₀: Input layer (2 neurons)
- L₁: Hidden layer (3 neurons)
- L₂: Output layer (1 neuron)

Activities:
- X₀ = ℝ²
- X₁ = ℝ³
- X₂ = ℝ

Weights:
- W₁₀: ℝ² → ℝ³ (6 parameters: 3×2 matrix)
- W₂₁: ℝ³ → ℝ (3 parameters: 1×3 matrix)

Weight functor:
- Π₀ = W₁₀ × W₂₁ (all weights)
- Π₁ = W₂₁ (weights for layers ≥ 1)
- Π₂ = ★ (no more weights)

Forgetting projections:
- Π₀₁: (w₁₀, w₂₁) ↦ w₂₁ (forget first layer weights)
- Π₁₂: w₂₁ ↦ ★ (forget second layer weights)
-}

-- Postulate ℝ for the example (would import from a real analysis library)
postulate
  ℝ : Type
  ℝ-is-set : is-set ℝ

-- Example: 2-layer MLP (input → hidden → output)
module Example-MLP where
  -- Chain with 2 edges (3 layers: L₀, L₁, L₂)
  mlp-oriented : OrientedGraph lzero lzero
  mlp-oriented = chain-graph 2

  open OrientedGraph mlp-oriented

  -- Activity spaces (concrete definitions as Fin n → ℝ)
  ℝ² : Type
  ℝ² = Fin 2 → ℝ

  ℝ³ : Type
  ℝ³ = Fin 3 → ℝ

  ℝ¹ : Type
  ℝ¹ = Fin 1 → ℝ

  -- Weight spaces (matrices as Fin m → Fin n → ℝ)
  Matrix-2×3 : Type
  Matrix-2×3 = Fin 2 → Fin 3 → ℝ  -- 2 input dims, 3 output dims

  Matrix-3×1 : Type
  Matrix-3×1 = Fin 3 → Fin 1 → ℝ  -- 3 input dims, 1 output dim

  -- Matrix-vector multiplication (requires summing)
  -- For now we postulate sum (would come from a linear algebra library)
  postulate
    sum : {n : Nat} → (Fin n → ℝ) → ℝ
    _*ᵣ_ : ℝ → ℝ → ℝ

  -- Weighted transformations (concrete matrix-vector multiplication)
  apply-w₁₀ : Matrix-2×3 → ℝ² → ℝ³
  apply-w₁₀ W x j = sum (λ i → W i j *ᵣ x i)

  apply-w₂₁ : Matrix-3×1 → ℝ³ → ℝ¹
  apply-w₂₁ W x j = sum (λ i → W i j *ᵣ x i)

  -- Activity spaces for each vertex
  mlp-activity-space : Fin 3 → Type
  mlp-activity-space = Fin-cases ℝ² (Fin-cases ℝ³ (Fin-cases ℝ¹ λ ()))

  -- Forward propagation: Edge is Σ[ k ∈ Fin 2 ] ..., so we pattern match on k
  -- Note: We use transport because mlp-activity-space doesn't compute definitionally
  mlp-forward : (w₁₀ : Matrix-2×3) → (w₂₁ : Matrix-3×1) →
                {i j : Vertex} → (e : Edge i j) →
                mlp-activity-space i → mlp-activity-space j
  mlp-forward w₁₀ w₂₁ {i} {j} (k , p , q) xᵢ =
      subst mlp-activity-space (sym q)
        (help k (subst mlp-activity-space p xᵢ))
      where
        help : (k : Fin 2) → mlp-activity-space (weaken k) → mlp-activity-space (fsuc k)
        help k with fin-view k
        ... | fv-zero = apply-w₁₀ w₁₀
        ... | fv-suc k' with fin-view k'
        ...   | fv-zero = apply-w₂₁ w₂₁

  -- Concrete Activity functor for the MLP
  mlp-activity : Matrix-2×3 → Matrix-3×1 → ActivityFunctor mlp-oriented
  mlp-activity w₁₀ w₂₁ = record
    { Activity = mlp-activity-space
    ; forward = mlp-forward w₁₀ w₂₁
    }

  -- Weight spaces: product of weights for all outgoing paths
  mlp-weight-product : Fin 3 → Type
  mlp-weight-product = Fin-cases (Matrix-2×3 × Matrix-3×1) (Fin-cases Matrix-3×1 (Fin-cases ⊤ λ ()))

  -- Forgetting projection: drop weights as we move forward
  mlp-forget : {i j : Vertex} → (e : Edge i j) →
               mlp-weight-product i → mlp-weight-product j
  mlp-forget {i} {j} (k , p , q) wᵢ =
      subst mlp-weight-product (sym q)
        (help k (subst mlp-weight-product p wᵢ))
      where
        help : (k : Fin 2) → mlp-weight-product (weaken k) → mlp-weight-product (fsuc k)
        help k with fin-view k
        ... | fv-zero = snd  -- Forget Matrix-2×3
        ... | fv-suc k' with fin-view k'
        ...   | fv-zero = λ _ → tt  -- Forget Matrix-3×1

  -- Concrete Weight functor for the MLP
  mlp-weights : WeightFunctor mlp-oriented
  mlp-weights = record
    { WeightSpace = λ {i} {j} e → mlp-weight-product i  -- Weight at source vertex
    ; Π = mlp-weight-product
    ; forget = mlp-forget
    }

{-|
## Computational vs Denotational: The Answer

The categorical approach is **BOTH**:

1. **Denotational Semantics**: The functors X^w, W, X give the *meaning*
   of the network architecture. They describe what each layer *is* (a set
   of possible states) and what each edge *means* (a transformation).
   This is independent of any particular execution.

2. **Computational Semantics**: The same functors also describe the
   *computation* during forward propagation. Following arrows in the
   category is literally computing the forward pass.

3. **Dynamic Semantics**: Natural transformations (like backpropagation
   in Section 1.4) describe how the system *evolves* during learning.

**The power**: By using category theory, we get a unified framework where:
- The graph Γ is the **syntax** (network architecture)
- The functors are the **semantics** (what it means)
- Natural transformations are the **dynamics** (how it changes)
- The topos C^ captures the **logic** of the network

This is why the paper says (Section 1.2):
> "It is remarkable that, in supervised learning, the Backpropagation
> algorithm is represented by a flow of natural transformations of the
> functor W to itself."

Backpropagation is NOT just an algorithm - it's a natural transformation!
-}
