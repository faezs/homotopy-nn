{-# OPTIONS --no-import-sorts #-}
{-|
# The Topos of a Deep Neural Network

**Reference**: Belfiore & Bennequin (2022), "Topos and Stacks of Deep Neural Networks"
arXiv:2106.14587v3 [math.AT]

Section 1: Architectures, Theorem 1.2 & Corollary (p.19)

## Main Result (Theorem 1.2 + Corollary):

Every DNN architecture defines a Grothendieck topos C∼ ≃ Sh(X, Alexandrov)
where X is a finite poset of trees with:
- **Minimal elements**: output layers + fork tips (vertices feeding multiple layers)
- **Maximal elements**: input layers + fork tangs (join points)
- **Alexandrov topology**: U open iff (y ∈ U, x ≤ y) → x ∈ U

## The Construction:

1. Network architecture = oriented classical graph Γ (Definition 1.1)
2. Add forks A★ → A for convergence points (Section 1.3)
3. Remove A★ to get poset X = CX (Proposition 1.1)
4. Topos = Sh(X) with Alexandrov coverage (Proposition 1.2)

## Key Properties (Section 1.5):

- **Localic topos**: generated by sub-singletons
- **Feed-forward**: presheaf X^w : X^op → Sets
- **Backpropagation**: flow of natural transformations W → W
- **Semantic functioning**: theories as objects in classifying topos

## Relationship to Our Work:

This is **complementary** to our neural codes approach (Manin & Marcolli):
- **This module**: Network *topology* (layers, connections) → topos
- **Neural.Base**: Neural *codes* (stimulus responses) → DirectedGraph as Functor ·⇉· FinSets
- Both frameworks analyze neural information from different perspectives
-}

module Neural.Topos.Architecture where

open import 1Lab.Prelude
open import 1Lab.HLevel
open import 1Lab.Path
open import 1Lab.Resizing
open import 1Lab.Path.IdentitySystem

open import Cat.Instances.Graphs using (Graph; Graph-hom)
open import Cat.Instances.Sheaves using (Sh[_,_]; Sheafification; Sheafification⊣ι)
open import Cat.Site.Base using (Coverage; forget-sheaf)
open import Cat.Diagram.Sieve
open import Cat.Functor.Base
open import Cat.Functor.Adjoint using (_⊣_)
open import Cat.Functor.Properties using (is-fully-faithful)
open import Cat.Diagram.Limit.Finite using (is-lex)
open import Cat.Base
open import Cat.Prelude

open import Topoi.Base using (Topos)
open import Order.Base using (Poset)
open import Order.Cat using (poset→category)

open import Data.Nat.Base using (Nat; zero; suc; suc-inj; zero≠suc)
open import Data.Fin.Base using (Fin; fzero; fsuc; fzero≠fsuc; lower)
open import Data.Dec.Base using (Dec; yes; no; Discrete)
open import Data.List.Base
open import Data.Sum.Base
open import 1Lab.Path.IdentitySystem using (Discrete→is-set)

private variable
  o ℓ : Level

{-|
## Definition 1.1: Oriented Classical Graph

From the paper (Section 1.1):

> "An **oriented graph** Γ is **directed** when the relation a ≤ b between vertices,
> defined by the existence of an oriented path, is a partial ordering on the set V(Γ).
> A graph is **classical** if there exists at most one edge between two vertices,
> and no loop at one vertex."

Properties:
- **Directed**: The transitive closure of the edge relation is a partial order
- **Classical**: At most one edge between any two vertices (proposition-valued)
- **No loops**: No edge from a vertex to itself

This is 1lab's Graph + additional constraints for DNN architectures.
-}
-- Helper: Edge path in a graph (defined before OrientedGraph)
-- Renamed to avoid clash with 1Lab.Path
module GraphPath (G : Graph o ℓ) where
  open Graph G

  data EdgePath : Vertex → Vertex → Type (o ⊔ ℓ) where
    path-nil  : ∀ {x} → EdgePath x x
    path-cons : ∀ {x y z} → Edge x y → EdgePath y z → EdgePath x z

record OrientedGraph (o ℓ : Level) : Type (lsuc o ⊔ lsuc ℓ) where
  field
    graph : Graph o ℓ

    -- Classical: at most one edge between vertices
    classical : ∀ {x y} → is-prop (graph .Graph.Edge x y)

    -- No self-loops (tadpoles)
    no-loops : ∀ {x} → ¬ (graph .Graph.Edge x x)

  -- Vertices are layers in the network
  Layer : Type o
  Layer = graph .Graph.Vertex

  -- Edges are direct connections between layers
  Connection : Layer → Layer → Type ℓ
  Connection = graph .Graph.Edge

  -- Edge path: sequence of connections
  open GraphPath graph public

  -- Reachability relation: x ≤ y iff exists edge path from x to y
  _≤ᴸ_ : Layer → Layer → Type (o ⊔ ℓ)
  x ≤ᴸ y = EdgePath x y

  field
    -- Directed: the reachability relation is a partial order (acyclic)
    -- This means: reflexive, transitive, antisymmetric (no oriented cycles)
    ≤-refl-ᴸ  : ∀ {x} → x ≤ᴸ x
    ≤-trans-ᴸ : ∀ {x y z} → x ≤ᴸ y → y ≤ᴸ z → x ≤ᴸ z
    ≤-antisym-ᴸ : ∀ {x y} → x ≤ᴸ y → y ≤ᴸ x → x ≡ y

  -- Input layers: no incoming edges
  is-input : Layer → Type (o ⊔ ℓ)
  is-input x = ∀ y → ¬ (Connection y x)

  -- Output layers: no outgoing edges
  is-output : Layer → Type (o ⊔ ℓ)
  is-output x = ∀ y → ¬ (Connection x y)

  -- Convergent vertex: multiple layers feed into it
  -- This is where we need to insert a fork (Section 1.3)

  -- Raw witness: specific pair of distinct incoming edges
  is-convergent-witness : Layer → Type (o ⊔ ℓ)
  is-convergent-witness a = Σ[ x ∈ Layer ] Σ[ y ∈ Layer ]
                            (¬ (x ≡ y)) × Connection x a × Connection y a

  -- Truncated: we only care IF it's convergent, not which edges
  is-convergent : Layer → Type (o ⊔ ℓ)
  is-convergent a = ∥ is-convergent-witness a ∥

{-|
## The Fork Construction (Section 1.3, Figure 1.2)

From the paper:

> "At each layer a where more than one layer sends information, say a', a'', ...
> we perform a surgery: introduce two new objects A★ and A, with arrows
> a' → A★, a'' → A★, ... and A★ → A, a → A"

**Purpose**: Convert the DAG into a poset by handling convergence points.

**Structure of a fork** (handle at a):
- Tips: a', a'', ... (the layers feeding into a)
- Star: A★ (join point for tips)
- Tang: A (transmission point)
- Handle: a (original convergent layer)

After this construction, only the tips can diverge (feed multiple points),
creating the tree structure described in Theorem 1.2.
-}
module _ (Γ : OrientedGraph o ℓ) where
  open OrientedGraph Γ

  -- Extended vertex set: original + fork stars + fork tangs
  data ForkVertex : Type (o ⊔ ℓ) where
    original   : Layer → ForkVertex
    fork-star  : (a : Layer) → (conv : is-convergent a) → ForkVertex
    fork-tang  : (a : Layer) → (conv : is-convergent a) → ForkVertex

  -- Edges in the forked graph (HIT with squash for cubical compatibility)
  data ForkEdge : ForkVertex → ForkVertex → Type (o ⊔ ℓ) where
    -- Original edges (not involving convergent points)
    orig-edge : ∀ {x y} → Connection x y → ¬ (is-convergent y) →
                ForkEdge (original x) (original y)

    -- Edges from tips to star: a' → A★
    tip-to-star : ∀ {x a} (conv : is-convergent a) → Connection x a →
                  ForkEdge (original x) (fork-star a conv)

    -- Edge from star to tang: A★ → A
    star-to-tang : ∀ {a} (conv : is-convergent a) →
                   ForkEdge (fork-star a conv) (fork-tang a conv)

    -- Edge from tang to handle: A → a
    tang-to-handle : ∀ {a} (conv : is-convergent a) →
                     ForkEdge (fork-tang a conv) (original a)

    -- HIT constructor: edges form a set (avoids pattern matching issues)
    ForkEdge-is-set : ∀ {x y} → is-set (ForkEdge x y)

  {-|
  ## Proving ForkVertex is a Set

  We prove ForkVertex is a set using Hedberg's theorem:
  1. Assume Layer has decidable equality (from finite network graphs)
  2. is-convergent is propositionally truncated
  3. Therefore ForkVertex has decidable equality
  4. By Hedberg: decidable equality → is-set
  -}

  module _ (Layer-dec : Discrete Layer) where
    open Discrete Layer-dec renaming (decide to _≟_)

    -- Discriminator to prove constructors are disjoint
    fork-tag : ForkVertex → Fin 3
    fork-tag (original _) = fzero
    fork-tag (fork-star _ _) = fsuc fzero
    fork-tag (fork-tang _ _) = fsuc (fsuc fzero)

    -- Decidable equality for ForkVertex
    ForkVertex-discrete : Discrete ForkVertex
    ForkVertex-discrete .Discrete.decide (original a) (original b) with a ≟ b
    ... | yes p = yes (ap original p)
    ... | no ¬p = no λ { q → ¬p (original-inj q) }
      where
        original-inj : original a ≡ original b → a ≡ b
        original-inj p = ap (λ { (original x) → x ; _ → a }) p
    ForkVertex-discrete .Discrete.decide (original a) (fork-star b _) =
      no λ { p → fzero≠fsuc (ap fork-tag p) }
    ForkVertex-discrete .Discrete.decide (original a) (fork-tang b _) =
      no λ { p → fzero≠fsuc (ap fork-tag p) }
    ForkVertex-discrete .Discrete.decide (fork-star a _) (original b) =
      no λ { p → fzero≠fsuc (sym (ap fork-tag p)) }
    ForkVertex-discrete .Discrete.decide (fork-star a p) (fork-star b q) with a ≟ b
    ... | yes a≡b =
      -- is-convergent is a proposition (truncated), so we can build path directly
      yes (λ i → fork-star (a≡b i) (is-prop→pathp (λ j → is-prop-∥-∥ {A = is-convergent-witness (a≡b j)}) p q i))
    ... | no ¬a≡b = no λ { r → ¬a≡b (fork-star-inj r) }
      where
        fork-star-inj : fork-star a p ≡ fork-star b q → a ≡ b
        fork-star-inj r = ap (λ { (fork-star x _) → x ; _ → a }) r
    ForkVertex-discrete .Discrete.decide (fork-star a _) (fork-tang b _) =
      no λ { p → zero≠suc (suc-inj (ap lower (ap fork-tag p))) }
    ForkVertex-discrete .Discrete.decide (fork-tang a _) (original b) =
      no λ { p → fzero≠fsuc (sym (ap fork-tag p)) }
    ForkVertex-discrete .Discrete.decide (fork-tang a _) (fork-star b _) =
      no λ { p → zero≠suc (suc-inj (ap lower (sym (ap fork-tag p)))) }
    ForkVertex-discrete .Discrete.decide (fork-tang a p) (fork-tang b q) with a ≟ b
    ... | yes a≡b =
      -- is-convergent is a proposition (truncated), so we can build path directly
      yes (λ i → fork-tang (a≡b i) (is-prop→pathp (λ j → is-prop-∥-∥ {A = is-convergent-witness (a≡b j)}) p q i))
    ... | no ¬a≡b = no λ { r → ¬a≡b (fork-tang-inj r) }
      where
        fork-tang-inj : fork-tang a p ≡ fork-tang b q → a ≡ b
        fork-tang-inj r = ap (λ { (fork-tang x _) → x ; _ → a }) r

    -- By Hedberg's theorem: discrete → is-set
    ForkVertex-is-set-proof : is-set ForkVertex
    ForkVertex-is-set-proof = Discrete→is-set ForkVertex-discrete

  -- Proof obligations for sets
  -- ForkVertex-is-set requires Layer-dec, keep as postulate for now
  postulate
    Layer-discrete : Discrete Layer  -- Assume finite graphs have decidable vertex equality

  ForkVertex-is-set : is-set ForkVertex
  ForkVertex-is-set = ForkVertex-is-set-proof Layer-discrete

  -- Note: ForkEdge-is-set is now a constructor of the HIT, not a separate proof

  -- The forked graph
  ForkGraph : Graph (o ⊔ ℓ) (o ⊔ ℓ)
  ForkGraph .Graph.Vertex = ForkVertex
  ForkGraph .Graph.Edge = ForkEdge
  ForkGraph .Graph.Vertex-is-set = ForkVertex-is-set
  ForkGraph .Graph.Edge-is-set = ForkEdge-is-set

  {-|
  ## Category C with Fork Vertices (Section 1.3)

  The category C includes A★ (fork-star) vertices, unlike poset X.
  This is the base category for defining the Grothendieck topology J.

  From the paper:
  > "With this category C, it is possible to define the analog of the
  > presheaves X^w, W=Π and X in general."
  -}

  {-|
  ## Ordering on ForkVertex as HIT

  We define _≤ᶠ_ as a Higher Inductive Type (HIT) with ONE path constructor
  for thinness (making it a proposition). Following 1Lab's pattern from Order/Cat.lagda.md,
  the identity and associativity laws are DERIVED lemmas, not HIT constructors.

  Key insight from 1Lab: When a relation is thin (a proposition), category laws
  are automatic: `cat .idr f = P.≤-thin _ _`, etc.
  -}

  data _≤ᶠ_ : ForkVertex → ForkVertex → Type (o ⊔ ℓ) where
    -- Identity morphism
    ≤ᶠ-refl : ∀ {x} → x ≤ᶠ x

    -- Original vertices follow the same ordering as in X
    ≤ᶠ-orig : ∀ {x y} → Connection x y → ¬ (is-convergent y) →
              original y ≤ᶠ original x

    -- Tang to handle (A → a)
    ≤ᶠ-tang-handle : ∀ {a} (conv : is-convergent a) →
                     original a ≤ᶠ fork-tang a conv

    -- Star to tang (A★ → A)
    ≤ᶠ-star-tang : ∀ {a} (conv : is-convergent a) →
                   fork-tang a conv ≤ᶠ fork-star a conv

    -- Tips to star (a' → A★)
    ≤ᶠ-tip-star : ∀ {a' a} (conv : is-convergent a) → Connection a' a →
                  fork-star a conv ≤ᶠ original a'

    -- Transitive closure (composition)
    ≤ᶠ-trans : ∀ {x y z} → x ≤ᶠ y → y ≤ᶠ z → x ≤ᶠ z

    -- ONLY path constructor: thinness (makes relation a proposition)
    -- Identity/associativity laws are DERIVED from this
    ≤ᶠ-thin-path : ∀ {x y} (p q : x ≤ᶠ y) → p ≡ q

  {-|
  ## Deriving Properties from Thinness

  The ≤ᶠ-thin-path constructor makes ≤ᶠ a proposition (at most one proof).
  From thinness, we derive:
  1. is-set property (propositions are sets)
  2. Category laws (identity, associativity) - automatic when Hom is a proposition
  3. Antisymmetry (from acyclic graph structure)

  This follows the pattern from 1Lab's Order/Cat.lagda.md.
  -}

  -- Thinness is exactly the thin-path constructor
  ≤ᶠ-thin : ∀ {x y} → is-prop (x ≤ᶠ y)
  ≤ᶠ-thin = ≤ᶠ-thin-path

  -- Props are automatically sets
  ≤ᶠ-is-set : ∀ {x y} → is-set (x ≤ᶠ y)
  ≤ᶠ-is-set = is-prop→is-set ≤ᶠ-thin

  -- Derived category laws using thinness (following Order/Cat.lagda.md pattern)
  ≤ᶠ-idl : ∀ {x y} (f : x ≤ᶠ y) → ≤ᶠ-trans ≤ᶠ-refl f ≡ f
  ≤ᶠ-idl f = ≤ᶠ-thin-path (≤ᶠ-trans ≤ᶠ-refl f) f

  ≤ᶠ-idr : ∀ {x y} (f : x ≤ᶠ y) → ≤ᶠ-trans f ≤ᶠ-refl ≡ f
  ≤ᶠ-idr f = ≤ᶠ-thin-path (≤ᶠ-trans f ≤ᶠ-refl) f

  ≤ᶠ-assoc : ∀ {w x y z} (h : w ≤ᶠ x) (g : x ≤ᶠ y) (f : y ≤ᶠ z)
           → ≤ᶠ-trans h (≤ᶠ-trans g f) ≡ ≤ᶠ-trans (≤ᶠ-trans h g) f
  ≤ᶠ-assoc h g f = ≤ᶠ-thin-path (≤ᶠ-trans h (≤ᶠ-trans g f)) (≤ᶠ-trans (≤ᶠ-trans h g) f)

  {-|
  ## Antisymmetry Proof

  To prove x ≤ᶠ y → y ≤ᶠ x → x ≡ y, we use proof irrelevance (≤ᶠ-thin).

  Key insight: Since ≤ᶠ is a proposition (by ≤ᶠ-thin), if both x ≤ᶠ y and y ≤ᶠ x hold,
  then x ≤ᶠ x also holds (by transitivity). By ForkVertex-is-set and the fact that
  the only non-trivial cycles would contradict the underlying acyclic graph,
  we must have x ≡ y.

  However, proving this requires induction on the structure of paths and using
  the antisymmetry of the underlying EdgePath relation from OrientedGraph.
  -}

  -- Antisymmetry: use eliminator-style proof to handle path constructors
  -- Pattern match only on point constructors; use is-set→squarep for path constructors
  ≤ᶠ-antisym : ∀ {x y} → x ≤ᶠ y → y ≤ᶠ x → x ≡ y
  ≤ᶠ-antisym ≤ᶠ-refl g = refl
  ≤ᶠ-antisym (≤ᶠ-orig conn₁ ¬conv₁) ≤ᶠ-refl = refl
  ≤ᶠ-antisym (≤ᶠ-orig conn₁ ¬conv₁) (≤ᶠ-orig conn₂ ¬conv₂) =
    ap original (≤-antisym-ᴸ (path-cons conn₂ path-nil) (path-cons conn₁ path-nil))
  ≤ᶠ-antisym (≤ᶠ-orig conn ¬conv) (≤ᶠ-trans g h) = {!!}
  ≤ᶠ-antisym (≤ᶠ-orig conn ¬conv) (≤ᶠ-thin-path g g₁ i) =
    ForkVertex-is-set _ _ (≤ᶠ-antisym (≤ᶠ-orig conn ¬conv) g) (≤ᶠ-antisym (≤ᶠ-orig conn ¬conv) g₁) i
  ≤ᶠ-antisym (≤ᶠ-tang-handle conv) g = {!!}
  ≤ᶠ-antisym (≤ᶠ-star-tang conv) g = {!!}
  ≤ᶠ-antisym (≤ᶠ-tip-star conv conn) g = {!!}
  ≤ᶠ-antisym (≤ᶠ-trans f f₁) g = {!!}
  ≤ᶠ-antisym (≤ᶠ-thin-path f f₁ i) g =
    ForkVertex-is-set _ _ (≤ᶠ-antisym f g) (≤ᶠ-antisym f₁ g) i

  -- Category C from fork poset
  Fork-Category : Precategory (o ⊔ ℓ) (o ⊔ ℓ)
  Fork-Category = po-cat where
    open Poset
    po : Poset (o ⊔ ℓ) (o ⊔ ℓ)
    po .Ob = ForkVertex
    po ._≤_ = _≤ᶠ_
    po .≤-thin = ≤ᶠ-thin
    po .≤-refl = ≤ᶠ-refl
    po .≤-trans = ≤ᶠ-trans
    po .≤-antisym = ≤ᶠ-antisym
    po-cat = poset→category po

  {-|
  ## Fork Topology J (Section 1.3)

  From the paper:
  > "the presheaf is a sheaf for a natural Grothendieck topology J on
  > the category C: in every object x of C the only covering is the
  > full category C|x, except when x is of the type of A★, where we
  > add the covering made by the arrows of the type a' → A★"

  **Key insight**: At A★ vertices, there are TWO covering sieves:
  1. The maximal sieve C|_{A★} (all morphisms into A★)
  2. The fork-tine sieve {a' → A★, a'' → A★, ...}↓

  At other vertices, only the maximal sieve covers.
  -}

  -- Predicate: is this vertex a fork-star A★?
  is-fork-star : ForkVertex → Bool
  is-fork-star (original _) = false
  is-fork-star (fork-star _ _) = true
  is-fork-star (fork-tang _ _) = false

  -- The fork topology J on C
  fork-coverage : Coverage Fork-Category (o ⊔ ℓ)
  fork-coverage = cov where
    open Coverage
    open Precategory Fork-Category
    open Sieve

    -- Number of coverings at each object
    cov : Coverage Fork-Category (o ⊔ ℓ)
    cov .covers x with is-fork-star x
    ... | true = Lift (o ⊔ ℓ) Bool  -- Two coverings at A★
    ... | false = Lift (o ⊔ ℓ) ⊤    -- One covering elsewhere

    cov .cover {x} i with is-fork-star x
    ... | false = maximal' {C = Fork-Category}
      -- Non-A★: only maximal sieve
    ... | true with Lift.lower i
    ...   | true  = maximal' {C = Fork-Category}
      -- A★: first covering is maximal sieve
    ...   | false = fork-tine-sieve x
      -- A★: second covering is fork-tine sieve
      where
        -- A morphism f: y → x is a tine if it's a direct connection a' → A★
        -- or if it factors through such a connection
        is-tine : {x y : Ob} → Hom y x → Ω
        is-tine (≤ᶠ-tip-star conv conn) = ⊤Ω  -- Direct tine: a' → fork-star
        is-tine (≤ᶠ-trans f g) = is-tine g  -- Downward closure: recursive call
        is-tine _ = ⊥Ω  -- Other morphisms are not tines

        -- Proof that tine property is downward closed
        -- By definition: is-tine (≤ᶠ-trans g f) = is-tine f (line 430)
        tine-closed : ∀ {x y z : Ob} (f : Hom z x) (g : Hom y z) →
                      ∣ is-tine f ∣ → ∣ is-tine (≤ᶠ-trans g f) ∣
        tine-closed f g hf = hf  -- By definition is-tine (≤ᶠ-trans g f) = is-tine f

        -- The sieve generated by fork tines a' → A★
        -- (downward closure of {a' → A★ | a' sends edge to convergent a})
        fork-tine-sieve : (x : Ob) → Sieve Fork-Category x
        fork-tine-sieve x .arrows f = is-tine f
        fork-tine-sieve x .closed {f = f} hf g = tine-closed f g hf

    -- Stability: pullback of covering sieve is covering
    -- ∃[ S ∈ covers V ] (cover S ⊆ pullback f (cover R))
    cov .stable {U} {V} R f = cov-stable R f
      where
        cov-stable : ∀ (R : cov .covers U) (f : Hom V U) →
                     ∃[ S ∈ cov .covers V ] (cov .cover S ⊆ pullback f (cov .cover R))
        cov-stable R g with is-fork-star U | is-fork-star V

        -- Case 1: U is not fork-star, V is not fork-star
        -- Covering on U is maximal, so pullback is maximal on V
        ... | false | false = inc (lift tt , λ h hf → tt)

        -- Case 2: U is not fork-star, V is fork-star
        -- Covering on U is maximal, pullback contains all arrows into V
        -- Use maximal covering on V (first covering, indexed by lift true)
        ... | false | true = inc (lift true , λ h hf → tt)

        -- Case 3: U is fork-star, V is not fork-star
        -- Two sub-cases depending on which covering R represents
        ... | true | false with Lift.lower R
        ...   | true = inc (lift tt , λ h hf → tt)  -- R is maximal on U
        ...   | false = inc (lift tt , λ h hf → tt) -- R is fork-tine on U, pullback to non-fork-star is still maximal

        -- Case 4: U is fork-star, V is fork-star (most complex)
        -- Need to handle both maximal and fork-tine coverings
        ... | true | true with Lift.lower R
        ...   | true = inc (lift true , λ h hf → tt)
              -- R is maximal on U, use maximal on V
        ...   | false = inc (lift false , λ h hf → hf)
              -- R is fork-tine on U, use fork-tine on V
              -- The pullback of a fork-tine sieve is a fork-tine sieve

  {-|
  ## The DNN Topos (Section 1.3)

  From the paper:

  > "The crossed product X of the X^w over W is defined as for the simple chains.
  > It is an object of the topos of sheaves over C that represents all the
  > possible functioning of the neural network."

  **Main Construction**: Topos = Sh[C, J] where:
  - C = Fork-Category (includes A★ vertices from fork construction)
  - J = fork-coverage (Grothendieck topology from Section 1.3)

  This is a **Grothendieck topos** with:

  **Objects**: Sheaves F : C^op → Sets where:
  - At A★: F(A★) ≅ ∏_{a'→A★} F(a') (sheaf condition for fork-tine covering)
  - At A (tang): F(A) receives product from F(A★)
  - At original vertices: standard presheaf values
  - Sheaf gluing respects fork structure

  **Key presheaves** (Section 1.3):
  - **X^w**: Feed-forward dynamics for fixed weights w
    - X^w(a) = activity states at layer a
    - X^w(A★) = X^w(A) = ∏ X^w(a') (product of incoming activities)
  - **W**: Weight presheaf
    - W(a) = ∏_{b∈Γ_a} W_b (weights on subgraph from a to outputs)
  - **X**: Crossed product X^w ×_W W (all possible functionings)

  **Backpropagation** (Theorem 1.1):
  - Flow of natural transformations W → W
  - Gradient descent in the topos
  -}

  -- The underlying precategory of sheaves on C with fork topology J
  -- This is the topos of sheaves described in Section 1.3
  DNN-Precategory : Precategory (lsuc (o ⊔ ℓ)) (o ⊔ ℓ)
  DNN-Precategory = Sh[ Fork-Category , fork-coverage ]

  -- Standard results from 1lab sheaf theory
  -- forget-sheaf is fully-faithful because Sheaves is defined as a full subcategory
  -- and the morphisms are definitionally equal to presheaf morphisms
  fork-forget-sheaf-ff : is-fully-faithful (forget-sheaf fork-coverage (o ⊔ ℓ))
  fork-forget-sheaf-ff = id-equiv
    -- The forgetful functor from sheaves to presheaves is the identity on morphisms,
    -- making it automatically fully-faithful

  -- TODO: Sheafification preserves finite limits (is left exact)
  -- This is a standard result in topos theory (Elephant A4.3.1, Johnstone)
  -- The proof requires showing that the HIT construction preserves terminals and pullbacks
  -- This is non-trivial but follows from the universal property of sheafification
  postulate
    fork-sheafification-lex : is-lex (Sheafification {C = Fork-Category} {J = fork-coverage})

  -- The DNN as a Grothendieck topos
  -- This is the topos Sh[C, J] from Section 1.3
  DNN-Topos : Topos {o = lsuc (o ⊔ ℓ)} (o ⊔ ℓ) DNN-Precategory
  DNN-Topos = record
    { site = Fork-Category
    ; ι = forget-sheaf fork-coverage (o ⊔ ℓ)
    ; has-ff = fork-forget-sheaf-ff
    ; L = Sheafification {C = Fork-Category} {J = fork-coverage}
    ; L-lex = fork-sheafification-lex
    ; L⊣ι = Sheafification⊣ι {C = Fork-Category} {J = fork-coverage}
    }

  {-|
  ## Section 1.4: Backpropagation as Natural Transformations

  From the paper (Theorem 1.1):
  > "Backpropagation is a flow of natural transformations of W, computed from
  > collections of singletons in X."

  **Key construction**:
  1. For vertex a, define Ω_a = set of directed paths from a to output layer
  2. Each path γ_a ∈ Ω_a gives a composed map φ_{γ_a}
  3. Cooperative sum: ⊕_{γ_a ∈ Ω_a} φ_{γ_a}
  4. Backprop formula (Lemma 1.1): dξₙ(δw_a) = Σ_{γ_a ∈ Ω_a} Π_{b_k ∈ γ_a} DX^{w₀}_{b_kB_k}
  -}

  -- A directed path from a to an output (uses is-output defined above)
  data DirectedPath (a : Layer) : Type (o ⊔ ℓ) where
    -- Base case: a itself is an output
    path-base : is-output a → DirectedPath a
    -- Inductive case: a → b, then path from b to output
    path-step : (b : Layer) → Connection a b → DirectedPath b → DirectedPath a

  -- The set Ω_a of all directed paths from a to outputs
  PathsFromVertex : (a : Layer) → Type (o ⊔ ℓ)
  PathsFromVertex a = DirectedPath a

  -- Extract the sequence of vertices along a path
  data PathVertices {a : Layer} : DirectedPath a → Type (o ⊔ ℓ) where
    vertices-base : {p : is-output a} → PathVertices (path-base p)
    vertices-step : {b : Layer} {conn : Connection a b} {path : DirectedPath b} →
                    PathVertices path → PathVertices (path-step b conn path)

  {-|
  ## Cooperative Sum (Section 1.4)

  From the paper:
  > "Two different elements γ', γ'' of Ω_a must coincide after a given vertex c,
  > where they join from different branches... we can define the sum φ_{γ'} ⊕ φ_{γ''}"

  The cooperative sum is associative and commutative over subsets of Ω_a representing
  trees embedded in the network.

  **Full cooperative sum** (Equation 1.7):
  ```
  ⊕_{γ_a ∈ Ω_a} φ_{γ_a} : X_A × (∏_{γ_a ∈ Ω_a} W_{aA}) → X_n
  ```
  -}

  postulate
    -- Placeholder for manifold structure on activity states
    ActivityManifold : Layer → Type (o ⊔ ℓ)

    -- Placeholder for weight spaces
    WeightSpace : (a b : Layer) → Connection a b → Type (o ⊔ ℓ)

  {-|
  ## Helper Types for Backpropagation

  These types capture the structure of paths through the network:
  - WeightProduct: Product of weight spaces along a directed path
  - OutputLayer: Extract the output layer reached by a path
  - CooperativeSumType: Combined map for multiple merging paths
  - PathDifferential: Tangent map along a path
  - GlobalInput/GlobalWeights: Global network state
  -}

  -- Product of weight spaces along a path
  WeightProduct : {a : Layer} → DirectedPath a → Type (o ⊔ ℓ)
  WeightProduct (path-base _) = Lift (o ⊔ ℓ) ⊤
  WeightProduct (path-step b conn rest) = WeightSpace _ b conn × WeightProduct rest

  -- Extract the output layer from a path
  OutputLayer : {a : Layer} → DirectedPath a → Layer
  OutputLayer {a} (path-base _) = a
  OutputLayer (path-step _ _ rest) = OutputLayer rest

  -- Type for cooperative sum over multiple paths
  CooperativeSumType : (a : Layer) → (paths : List (DirectedPath a)) → Type (o ⊔ ℓ)
  CooperativeSumType a [] = Lift (o ⊔ ℓ) ⊤
  CooperativeSumType a (p ∷ ps) =
    (ActivityManifold a → WeightProduct p → ActivityManifold (OutputLayer p)) ×
    CooperativeSumType a ps

  -- Differential along a path (tangent map composition)
  PathDifferential : {a : Layer} →
                    (path : DirectedPath a) →
                    WeightProduct path →
                    Type (o ⊔ ℓ)
  PathDifferential {a} path w =
    (x : ActivityManifold a) →
    (δw : WeightProduct path) →
    ActivityManifold (OutputLayer path)

  -- Global input at initial layer (for networks with single input)
  GlobalInput : Type (o ⊔ ℓ)
  GlobalInput = Σ[ a ∈ Layer ] (is-input a × ActivityManifold a)

  -- Global weight assignment for entire network
  GlobalWeights : Type (o ⊔ ℓ)
  GlobalWeights = Σ[ a ∈ Layer ] Σ[ b ∈ Layer ] Σ[ conn ∈ Connection a b ] WeightSpace a b conn

  postulate
    -- The map along a path for fixed weights
    φ-path : {a : Layer} →
             (path : DirectedPath a) →
             (weights : WeightProduct path) →
             ActivityManifold a → ActivityManifold (OutputLayer path)

    -- Cooperative sum: combines paths that merge
    cooperative-sum : {a : Layer} →
                     (paths : List (DirectedPath a)) →
                     CooperativeSumType a paths

  {-|
  ## Backpropagation Formula (Lemma 1.1)

  From the paper (Equation 1.10):
  ```
  dξₙ(δw_a) = Σ_{γ_a ∈ Ω_a} Π_{b_k ∈ γ_a} DX^{w₀}_{b_kB_k} ∘ Dρ_{B_kb_{k-1}} ∘ ∂_wX^w_{aA}.δw_a
  ```

  This gives a linear map from T_{w₀}(W_a) to T_{ξ₀}(X_n).

  Composing with dF (the gradient of the loss function) and applying the Riemannian
  metric gives the vector field β(w₀|ξ₀).
  -}

  postulate
    -- Tangent bundle of activity manifold
    TangentActivity : (a : Layer) → ActivityManifold a → Type (o ⊔ ℓ)

    -- Tangent bundle of weight space
    TangentWeight : {a b : Layer} → (conn : Connection a b) →
                   WeightSpace a b conn → Type (o ⊔ ℓ)

    -- Differential of the network map along a path
    D-path-map : {a : Layer} →
                (path : DirectedPath a) →
                (w : WeightProduct path) →
                PathDifferential path w

    -- Backpropagation differential (Lemma 1.1)
    backprop-differential : {a : Layer} →
                           (ξ₀ : GlobalInput) →
                           (w₀ : GlobalWeights) →
                           (x : ActivityManifold a) →
                           TangentActivity a x

  {-|
  ## Theorem 1.1: Backpropagation as Natural Transformation

  From the paper:
  > "Backpropagation is a flow of natural transformations of W, computed from
  > collections of singletons in X."

  The gradient flow β integrates to a one-parameter family of natural transformations
  of the weight presheaf W.
  -}

  postulate
    -- The weight presheaf W : C^op → Sets
    -- (Presheaf on Fork-Category with values in activity/weight spaces)
    WeightPresheaf : Functor (Fork-Category ^op) (Sets (o ⊔ ℓ))

    -- Activity presheaf X^w for fixed weights
    ActivityPresheaf : GlobalWeights → Functor (Fork-Category ^op) (Sets (o ⊔ ℓ))

    -- Natural transformation: gradient flow on weights
    -- This is the key result of Theorem 1.1
    BackpropagationFlow : (w : GlobalWeights) → WeightPresheaf => ActivityPresheaf w

{-|
## Theorem 1.2: The Poset X of a DNN (Section 1.5)

From the paper:

> "The poset X of a DNN is made by a finite number of trees, rooted in the
> maximal points and which are joined in the minimal points."

**Minimal elements** (arrows point TO these):
- Output layers (terminal in Γ)
- Tips a' (vertices that feed into forks)

**Maximal elements** (arrows point FROM these):
- Input layers (initial in Γ)
- Tangs A (fork join points)

**Key property**: After removing A★, we get a poset where the category C_X is
opposite to the free category on the forked graph.
-}
module _ (Γ : OrientedGraph o ℓ) where
  open OrientedGraph Γ

  -- The poset X is ForkVertex minus the stars (Section 1.5, Proposition 1.1)
  data X-Vertex : Type (o ⊔ ℓ) where
    x-original   : Layer → X-Vertex
    x-fork-tang  : (a : Layer) → is-convergent a → X-Vertex

  -- Prove X-Vertex is a set (same approach as ForkVertex)
  module _ (Layer-dec : Discrete Layer) where
    open Discrete Layer-dec renaming (decide to _≟_)

    X-Vertex-discrete : Discrete X-Vertex
    X-Vertex-discrete .Discrete.decide (x-original a) (x-original b) with a ≟ b
    ... | yes p = yes (ap x-original p)
    ... | no ¬p = no λ { q → ¬p (x-original-inj q) }
      where
        x-original-inj : x-original a ≡ x-original b → a ≡ b
        x-original-inj p = ap (λ { (x-original x) → x ; _ → a }) p
    X-Vertex-discrete .Discrete.decide (x-original a) (x-fork-tang b _) =
      no λ { p → fzero≠fsuc (ap x-tag p) }
      where
        x-tag : X-Vertex → Fin 2
        x-tag (x-original _) = fzero
        x-tag (x-fork-tang _ _) = fsuc fzero
    X-Vertex-discrete .Discrete.decide (x-fork-tang a _) (x-original b) =
      no λ { p → fzero≠fsuc (sym (ap x-tag p)) }
      where
        x-tag : X-Vertex → Fin 2
        x-tag (x-original _) = fzero
        x-tag (x-fork-tang _ _) = fsuc fzero
    X-Vertex-discrete .Discrete.decide (x-fork-tang a p) (x-fork-tang b q) with a ≟ b
    ... | yes a≡b =
      yes (λ i → x-fork-tang (a≡b i) (is-prop→pathp (λ j → is-prop-∥-∥ {A = is-convergent-witness (a≡b j)}) p q i))
    ... | no ¬a≡b = no λ { r → ¬a≡b (x-fork-tang-inj r) }
      where
        x-fork-tang-inj : x-fork-tang a p ≡ x-fork-tang b q → a ≡ b
        x-fork-tang-inj r = ap (λ { (x-fork-tang x _) → x ; _ → a }) r

    X-Vertex-is-set-proof : is-set X-Vertex
    X-Vertex-is-set-proof = Discrete→is-set X-Vertex-discrete

  X-Vertex-is-set : is-set X-Vertex
  X-Vertex-is-set = X-Vertex-is-set-proof Layer-discrete

  -- Ordering on X: paths in the forked graph (excluding stars)
  -- Arrows go OPPOSITE to information flow (categorical convention)
  -- Defined as HIT with path constructors (same approach as _≤ᶠ_)
  data _≤ˣ_ : X-Vertex → X-Vertex → Type (o ⊔ ℓ) where
    -- Reflexivity
    ≤ˣ-refl : ∀ {x} → x ≤ˣ x

    -- Original edge (non-convergent): y ≤ x (arrow x → y in Γ)
    ≤ˣ-orig : ∀ {x y} → Connection x y → ¬ (is-convergent y) →
              x-original y ≤ˣ x-original x

    -- Tang to handle: a ≤ A (arrow A → a in forked graph)
    ≤ˣ-tang-handle : ∀ {a} (conv : is-convergent a) →
                     x-original a ≤ˣ x-fork-tang a conv

    -- Tip to tang: a' ≤ A (path a' → A★ → A in forked graph)
    ≤ˣ-tip-tang : ∀ {a' a} (conv : is-convergent a) → Connection a' a →
                  x-fork-tang a conv ≤ˣ x-original a'

    -- Transitivity
    ≤ˣ-trans : ∀ {x y z} → x ≤ˣ y → y ≤ˣ z → x ≤ˣ z

    -- Path constructors for quotient (HIT)
    ≤ˣ-idl : ∀ {x y} (f : x ≤ˣ y) → ≤ˣ-trans ≤ˣ-refl f ≡ f
    ≤ˣ-idr : ∀ {x y} (f : x ≤ˣ y) → ≤ˣ-trans f ≤ˣ-refl ≡ f
    ≤ˣ-assoc : ∀ {w x y z} (f : y ≤ˣ z) (g : x ≤ˣ y) (h : w ≤ˣ x)
             → ≤ˣ-trans f (≤ˣ-trans g h) ≡ ≤ˣ-trans (≤ˣ-trans f g) h

    -- Set truncation
    ≤ˣ-is-set : ∀ {x y} → is-set (x ≤ˣ y)

  -- Poset axioms derived from HIT
  ≤ˣ-thin : ∀ {x y} → is-prop (x ≤ˣ y)
  ≤ˣ-thin p q = ≤ˣ-is-set p q

  -- Antisymmetry proof (same structure as ≤ᶠ-antisym)
  ≤ˣ-antisym : ∀ {x y} → x ≤ˣ y → y ≤ˣ x → x ≡ y
  ≤ˣ-antisym ≤ˣ-refl _ = refl
  ≤ˣ-antisym _ ≤ˣ-refl = refl
  ≤ˣ-antisym (≤ˣ-orig conn₁ ¬conv₁) (≤ˣ-orig conn₂ ¬conv₂) =
    ap x-original (≤-antisym-ᴸ (path-cons conn₂ path-nil) (path-cons conn₁ path-nil))
  ≤ˣ-antisym (≤ˣ-tang-handle _) (≤ˣ-tang-handle _) = refl
  ≤ˣ-antisym (≤ˣ-tip-tang _ _) (≤ˣ-tip-tang _ _) = refl
  ≤ˣ-antisym (≤ˣ-trans f g) h = ≤ˣ-antisym g (≤ˣ-trans h f)
  ≤ˣ-antisym f (≤ˣ-trans g h) = ≤ˣ-antisym (≤ˣ-trans f g) h
  ≤ˣ-antisym (≤ˣ-idl f i) g = ≤ˣ-antisym f g
  ≤ˣ-antisym f (≤ˣ-idl g i) = ≤ˣ-antisym f g
  ≤ˣ-antisym (≤ˣ-idr f i) g = ≤ˣ-antisym f g
  ≤ˣ-antisym f (≤ˣ-idr g i) = ≤ˣ-antisym f g
  ≤ˣ-antisym (≤ˣ-assoc f g h i) k = ≤ˣ-antisym (≤ˣ-trans f (≤ˣ-trans g h)) k
  ≤ˣ-antisym f (≤ˣ-assoc g h k i) = ≤ˣ-antisym f (≤ˣ-trans g (≤ˣ-trans h k))
  ≤ˣ-antisym (≤ˣ-is-set p q r s i j) g = X-Vertex-is-set _ _ (≤ˣ-antisym p g) (≤ˣ-antisym q g) i j
  ≤ˣ-antisym f (≤ˣ-is-set p q r s i j) = X-Vertex-is-set _ _ (≤ˣ-antisym f p) (≤ˣ-antisym f q) i j

  -- The poset X (this is C_X in the paper)
  X-Poset : Poset (o ⊔ ℓ) (o ⊔ ℓ)
  X-Poset = poset where
    open Poset
    poset : Poset (o ⊔ ℓ) (o ⊔ ℓ)
    poset .Ob = X-Vertex
    poset ._≤_ = _≤ˣ_
    poset .≤-thin = ≤ˣ-thin
    poset .≤-refl = ≤ˣ-refl
    poset .≤-trans = ≤ˣ-trans
    poset .≤-antisym = ≤ˣ-antisym

  -- Convert poset to category (standard construction)
  -- Objects: X-Vertex
  -- Hom(x,y): x ≤ y (proposition, so ≤1 morphism)
  X-Category : Precategory (o ⊔ ℓ) (o ⊔ ℓ)
  X-Category = cat where
    open Poset X-Poset
    cat : Precategory (o ⊔ ℓ) (o ⊔ ℓ)
    cat .Precategory.Ob = X-Vertex
    cat .Precategory.Hom x y = x ≤ˣ y
    cat .Precategory.Hom-set x y = is-prop→is-set ≤-thin
    cat .Precategory.id = ≤ˣ-refl
    cat .Precategory._∘_ = flip ≤ˣ-trans
    cat .Precategory.idr _ = ≤-thin _ _
    cat .Precategory.idl _ = ≤-thin _ _
    cat .Precategory.assoc _ _ _ = ≤-thin _ _

  {-|
  ## Alexandrov (Lower) Topology (Section 1.5, Definitions 1)

  From the paper:

  > "The (lower) Alexandrov topology on X is made by subsets U of X such that
  > (y ∈ U and x ≤ y) imply x ∈ U."

  **Basis**: For each α ∈ X, the principal ideal ↓α = { β | β ≤ α }

  **Geometric interpretation**: Information flows "downward" from inputs to outputs.
  In the categorical direction (opposite to graph edges), this is the upward closure.

  **Sheaf condition**: A presheaf F on X is a sheaf iff F(U) ≅ lim_{x ∈ U} F(x)
  for every open U.

  For the DNN topos, the Grothendieck topology is given by covering sieves
  that respect this Alexandrov structure.
  -}
