{-# OPTIONS --no-import-sorts --allow-unsolved-metas #-}
{-|
# The Topos of a Deep Neural Network

**Reference**: Belfiore & Bennequin (2022), "Topos and Stacks of Deep Neural Networks"
arXiv:2106.14587v3 [math.AT]

Section 1: Architectures, Theorem 1.2 & Corollary (p.19)

## Main Result (Theorem 1.2 + Corollary):

Every DNN architecture defines a Grothendieck topos C∼ ≃ Sh(X, Alexandrov)
where X is a finite poset of trees with:
- **Minimal elements**: output layers + fork tips (vertices feeding multiple layers)
- **Maximal elements**: input layers + fork tangs (join points)
- **Alexandrov topology**: U open iff (y ∈ U, x ≤ y) → x ∈ U

## The Construction:

1. Network architecture = oriented classical graph Γ (Definition 1.1)
2. Add forks A★ → A for convergence points (Section 1.3)
3. Remove A★ to get poset X = CX (Proposition 1.1)
4. Topos = Sh(X) with Alexandrov coverage (Proposition 1.2)

## Key Properties (Section 1.5):

- **Localic topos**: generated by sub-singletons
- **Feed-forward**: presheaf X^w : X^op → Sets
- **Backpropagation**: flow of natural transformations W → W
- **Semantic functioning**: theories as objects in classifying topos

## Relationship to Our Work:

This is **complementary** to our neural codes approach (Manin & Marcolli):
- **This module**: Network *topology* (layers, connections) → topos
- **Neural.Base**: Neural *codes* (stimulus responses) → DirectedGraph as Functor ·⇉· FinSets
- Both frameworks analyze neural information from different perspectives
-}

module Neural.Topos.Architecture where

open import 1Lab.Prelude
open import 1Lab.HLevel
open import 1Lab.Path
open import 1Lab.Resizing

open import Cat.Instances.Graphs using (Graph; Graph-hom)
open import Cat.Instances.Sheaves using (Sh[_,_]; Sheafification; Sheafification⊣ι)
open import Cat.Site.Base using (Coverage; forget-sheaf)
open import Cat.Diagram.Sieve
open import Cat.Functor.Base
open import Cat.Functor.Adjoint using (_⊣_)
open import Cat.Functor.Properties using (is-fully-faithful)
open import Cat.Diagram.Limit.Finite using (is-lex)
open import Cat.Base
open import Cat.Prelude

open import Topoi.Base using (Topos)
open import Order.Base using (Poset)
open import Order.Cat using (poset→category)

open import Data.Nat.Base using (Nat; zero; suc)
open import Data.Fin.Base using (Fin)
open import Data.List.Base
open import Data.Sum.Base

private variable
  o ℓ : Level

{-|
## Definition 1.1: Oriented Classical Graph

From the paper (Section 1.1):

> "An **oriented graph** Γ is **directed** when the relation a ≤ b between vertices,
> defined by the existence of an oriented path, is a partial ordering on the set V(Γ).
> A graph is **classical** if there exists at most one edge between two vertices,
> and no loop at one vertex."

Properties:
- **Directed**: The transitive closure of the edge relation is a partial order
- **Classical**: At most one edge between any two vertices (proposition-valued)
- **No loops**: No edge from a vertex to itself

This is 1lab's Graph + additional constraints for DNN architectures.
-}
-- Helper: Edge path in a graph (defined before OrientedGraph)
-- Renamed to avoid clash with 1Lab.Path
module GraphPath (G : Graph o ℓ) where
  open Graph G

  data EdgePath : Vertex → Vertex → Type (o ⊔ ℓ) where
    path-nil  : ∀ {x} → EdgePath x x
    path-cons : ∀ {x y z} → Edge x y → EdgePath y z → EdgePath x z

record OrientedGraph (o ℓ : Level) : Type (lsuc o ⊔ lsuc ℓ) where
  field
    graph : Graph o ℓ

    -- Classical: at most one edge between vertices
    classical : ∀ {x y} → is-prop (graph .Graph.Edge x y)

    -- No self-loops (tadpoles)
    no-loops : ∀ {x} → ¬ (graph .Graph.Edge x x)

  -- Vertices are layers in the network
  Layer : Type o
  Layer = graph .Graph.Vertex

  -- Edges are direct connections between layers
  Connection : Layer → Layer → Type ℓ
  Connection = graph .Graph.Edge

  -- Edge path: sequence of connections
  open GraphPath graph public

  -- Reachability relation: x ≤ y iff exists edge path from x to y
  _≤ᴸ_ : Layer → Layer → Type (o ⊔ ℓ)
  x ≤ᴸ y = EdgePath x y

  field
    -- Directed: the reachability relation is a partial order (acyclic)
    -- This means: reflexive, transitive, antisymmetric (no oriented cycles)
    ≤-refl-ᴸ  : ∀ {x} → x ≤ᴸ x
    ≤-trans-ᴸ : ∀ {x y z} → x ≤ᴸ y → y ≤ᴸ z → x ≤ᴸ z
    ≤-antisym-ᴸ : ∀ {x y} → x ≤ᴸ y → y ≤ᴸ x → x ≡ y

  -- Input layers: no incoming edges
  is-input : Layer → Type (o ⊔ ℓ)
  is-input x = ∀ y → ¬ (Connection y x)

  -- Output layers: no outgoing edges
  is-output : Layer → Type (o ⊔ ℓ)
  is-output x = ∀ y → ¬ (Connection x y)

  -- Convergent vertex: multiple layers feed into it
  -- This is where we need to insert a fork (Section 1.3)
  is-convergent : Layer → Type (o ⊔ ℓ)
  is-convergent a = Σ[ x ∈ Layer ] Σ[ y ∈ Layer ]
                    (¬ (x ≡ y)) × Connection x a × Connection y a

{-|
## The Fork Construction (Section 1.3, Figure 1.2)

From the paper:

> "At each layer a where more than one layer sends information, say a', a'', ...
> we perform a surgery: introduce two new objects A★ and A, with arrows
> a' → A★, a'' → A★, ... and A★ → A, a → A"

**Purpose**: Convert the DAG into a poset by handling convergence points.

**Structure of a fork** (handle at a):
- Tips: a', a'', ... (the layers feeding into a)
- Star: A★ (join point for tips)
- Tang: A (transmission point)
- Handle: a (original convergent layer)

After this construction, only the tips can diverge (feed multiple points),
creating the tree structure described in Theorem 1.2.
-}
module _ (Γ : OrientedGraph o ℓ) where
  open OrientedGraph Γ

  -- Extended vertex set: original + fork stars + fork tangs
  data ForkVertex : Type (o ⊔ ℓ) where
    original   : Layer → ForkVertex
    fork-star  : (a : Layer) → (conv : is-convergent a) → ForkVertex
    fork-tang  : (a : Layer) → (conv : is-convergent a) → ForkVertex

  -- Edges in the forked graph
  data ForkEdge : ForkVertex → ForkVertex → Type (o ⊔ ℓ) where
    -- Original edges (not involving convergent points)
    orig-edge : ∀ {x y} → Connection x y → ¬ (is-convergent y) →
                ForkEdge (original x) (original y)

    -- Edges from tips to star: a' → A★
    tip-to-star : ∀ {x a} (conv : is-convergent a) → Connection x a →
                  ForkEdge (original x) (fork-star a conv)

    -- Edge from star to tang: A★ → A
    star-to-tang : ∀ {a} (conv : is-convergent a) →
                   ForkEdge (fork-star a conv) (fork-tang a conv)

    -- Edge from tang to handle: A → a
    tang-to-handle : ∀ {a} (conv : is-convergent a) →
                     ForkEdge (fork-tang a conv) (original a)

  -- Proof obligations for sets
  postulate
    ForkVertex-is-set : is-set ForkVertex
    ForkEdge-is-set : ∀ {x y} → is-set (ForkEdge x y)

  -- The forked graph
  ForkGraph : Graph (o ⊔ ℓ) (o ⊔ ℓ)
  ForkGraph .Graph.Vertex = ForkVertex
  ForkGraph .Graph.Edge = ForkEdge
  ForkGraph .Graph.Vertex-is-set = ForkVertex-is-set
  ForkGraph .Graph.Edge-is-set = ForkEdge-is-set

  {-|
  ## Category C with Fork Vertices (Section 1.3)

  The category C includes A★ (fork-star) vertices, unlike poset X.
  This is the base category for defining the Grothendieck topology J.

  From the paper:
  > "With this category C, it is possible to define the analog of the
  > presheaves X^w, W=Π and X in general."
  -}

  -- Ordering on ForkVertex (includes A★)
  data _≤ᶠ_ : ForkVertex → ForkVertex → Type (o ⊔ ℓ) where
    ≤ᶠ-refl : ∀ {x} → x ≤ᶠ x

    -- Original vertices follow the same ordering as in X
    ≤ᶠ-orig : ∀ {x y} → Connection x y → ¬ (is-convergent y) →
              original y ≤ᶠ original x

    -- Tang to handle (A → a)
    ≤ᶠ-tang-handle : ∀ {a} (conv : is-convergent a) →
                     original a ≤ᶠ fork-tang a conv

    -- Star to tang (A★ → A)
    ≤ᶠ-star-tang : ∀ {a} (conv : is-convergent a) →
                   fork-tang a conv ≤ᶠ fork-star a conv

    -- Tips to star (a' → A★)
    ≤ᶠ-tip-star : ∀ {a' a} (conv : is-convergent a) → Connection a' a →
                  fork-star a conv ≤ᶠ original a'

    -- Transitivity
    ≤ᶠ-trans : ∀ {x y z} → x ≤ᶠ y → y ≤ᶠ z → x ≤ᶠ z

  -- Category C from fork poset
  Fork-Category : Precategory (o ⊔ ℓ) (o ⊔ ℓ)
  Fork-Category = po-cat where
    open Poset
    po : Poset (o ⊔ ℓ) (o ⊔ ℓ)
    po .Ob = ForkVertex
    po ._≤_ = _≤ᶠ_
    po .≤-thin = postulate-≤ᶠ-thin where postulate postulate-≤ᶠ-thin : ∀ {x y} → is-prop (x ≤ᶠ y)
    po .≤-refl = ≤ᶠ-refl
    po .≤-trans = ≤ᶠ-trans
    po .≤-antisym = postulate-≤ᶠ-antisym where postulate postulate-≤ᶠ-antisym : ∀ {x y} → x ≤ᶠ y → y ≤ᶠ x → x ≡ y
    po-cat = poset→category po

  {-|
  ## Fork Topology J (Section 1.3)

  From the paper:
  > "the presheaf is a sheaf for a natural Grothendieck topology J on
  > the category C: in every object x of C the only covering is the
  > full category C|x, except when x is of the type of A★, where we
  > add the covering made by the arrows of the type a' → A★"

  **Key insight**: At A★ vertices, there are TWO covering sieves:
  1. The maximal sieve C|_{A★} (all morphisms into A★)
  2. The fork-tine sieve {a' → A★, a'' → A★, ...}↓

  At other vertices, only the maximal sieve covers.
  -}

  -- Predicate: is this vertex a fork-star A★?
  is-fork-star : ForkVertex → Bool
  is-fork-star (original _) = false
  is-fork-star (fork-star _ _) = true
  is-fork-star (fork-tang _ _) = false

  -- The fork topology J on C
  -- NOTE: We postulate the entire coverage due to complexity of proving stability
  postulate fork-coverage : Coverage Fork-Category (o ⊔ ℓ)

  {-
  -- Sketch of the construction (for reference):
  -- At fork-star vertices A★, there are two coverings:
  -- 1. The maximal sieve (as usual)
  -- 2. The fork-tine sieve (connections a' → A★ from inputs)
  --
  -- At other vertices, only the maximal sieve covers.
  --
  -- The stability property (pullbacks preserve coverings) holds because:
  -- - For maximal sieves: always stable
  -- - For fork-tine sieves: pullback along f gives tines at source of f
  -}

  {-|
  ## The DNN Topos (Section 1.3)

  From the paper:

  > "The crossed product X of the X^w over W is defined as for the simple chains.
  > It is an object of the topos of sheaves over C that represents all the
  > possible functioning of the neural network."

  **Main Construction**: Topos = Sh[C, J] where:
  - C = Fork-Category (includes A★ vertices from fork construction)
  - J = fork-coverage (Grothendieck topology from Section 1.3)

  This is a **Grothendieck topos** with:

  **Objects**: Sheaves F : C^op → Sets where:
  - At A★: F(A★) ≅ ∏_{a'→A★} F(a') (sheaf condition for fork-tine covering)
  - At A (tang): F(A) receives product from F(A★)
  - At original vertices: standard presheaf values
  - Sheaf gluing respects fork structure

  **Key presheaves** (Section 1.3):
  - **X^w**: Feed-forward dynamics for fixed weights w
    - X^w(a) = activity states at layer a
    - X^w(A★) = X^w(A) = ∏ X^w(a') (product of incoming activities)
  - **W**: Weight presheaf
    - W(a) = ∏_{b∈Γ_a} W_b (weights on subgraph from a to outputs)
  - **X**: Crossed product X^w ×_W W (all possible functionings)

  **Backpropagation** (Theorem 1.1):
  - Flow of natural transformations W → W
  - Gradient descent in the topos
  -}

  -- The underlying precategory of sheaves on C with fork topology J
  -- This is the topos of sheaves described in Section 1.3
  DNN-Precategory : Precategory (lsuc (o ⊔ ℓ)) (o ⊔ ℓ)
  DNN-Precategory = Sh[ Fork-Category , fork-coverage ]

  -- Standard results that should be in 1lab (or proven separately)
  postulate
    fork-forget-sheaf-ff : is-fully-faithful (forget-sheaf fork-coverage (o ⊔ ℓ))
    fork-sheafification-lex : is-lex (Sheafification {C = Fork-Category} {J = fork-coverage})

  -- The DNN as a Grothendieck topos
  -- This is the topos Sh[C, J] from Section 1.3
  DNN-Topos : Topos {o = lsuc (o ⊔ ℓ)} (o ⊔ ℓ) DNN-Precategory
  DNN-Topos = record
    { site = Fork-Category
    ; ι = forget-sheaf fork-coverage (o ⊔ ℓ)
    ; has-ff = fork-forget-sheaf-ff
    ; L = Sheafification {C = Fork-Category} {J = fork-coverage}
    ; L-lex = fork-sheafification-lex
    ; L⊣ι = Sheafification⊣ι {C = Fork-Category} {J = fork-coverage}
    }

  {-|
  ## Section 1.4: Backpropagation as Natural Transformations

  From the paper (Theorem 1.1):
  > "Backpropagation is a flow of natural transformations of W, computed from
  > collections of singletons in X."

  **Key construction**:
  1. For vertex a, define Ω_a = set of directed paths from a to output layer
  2. Each path γ_a ∈ Ω_a gives a composed map φ_{γ_a}
  3. Cooperative sum: ⊕_{γ_a ∈ Ω_a} φ_{γ_a}
  4. Backprop formula (Lemma 1.1): dξₙ(δw_a) = Σ_{γ_a ∈ Ω_a} Π_{b_k ∈ γ_a} DX^{w₀}_{b_kB_k}
  -}

  -- A directed path from a to an output (uses is-output defined above)
  data DirectedPath (a : Layer) : Type (o ⊔ ℓ) where
    -- Base case: a itself is an output
    path-base : is-output a → DirectedPath a
    -- Inductive case: a → b, then path from b to output
    path-step : (b : Layer) → Connection a b → DirectedPath b → DirectedPath a

  -- The set Ω_a of all directed paths from a to outputs
  PathsFromVertex : (a : Layer) → Type (o ⊔ ℓ)
  PathsFromVertex a = DirectedPath a

  -- Extract the sequence of vertices along a path
  data PathVertices {a : Layer} : DirectedPath a → Type (o ⊔ ℓ) where
    vertices-base : {p : is-output a} → PathVertices (path-base p)
    vertices-step : {b : Layer} {conn : Connection a b} {path : DirectedPath b} →
                    PathVertices path → PathVertices (path-step b conn path)

  {-|
  ## Cooperative Sum (Section 1.4)

  From the paper:
  > "Two different elements γ', γ'' of Ω_a must coincide after a given vertex c,
  > where they join from different branches... we can define the sum φ_{γ'} ⊕ φ_{γ''}"

  The cooperative sum is associative and commutative over subsets of Ω_a representing
  trees embedded in the network.

  **Full cooperative sum** (Equation 1.7):
  ```
  ⊕_{γ_a ∈ Ω_a} φ_{γ_a} : X_A × (∏_{γ_a ∈ Ω_a} W_{aA}) → X_n
  ```
  -}

  postulate
    -- Placeholder for manifold structure on activity states
    ActivityManifold : Layer → Type (o ⊔ ℓ)

    -- Placeholder for weight spaces
    WeightSpace : (a b : Layer) → Connection a b → Type (o ⊔ ℓ)

  {-|
  ## Helper Types for Backpropagation

  These types capture the structure of paths through the network:
  - WeightProduct: Product of weight spaces along a directed path
  - OutputLayer: Extract the output layer reached by a path
  - CooperativeSumType: Combined map for multiple merging paths
  - PathDifferential: Tangent map along a path
  - GlobalInput/GlobalWeights: Global network state
  -}

  -- Product of weight spaces along a path
  WeightProduct : {a : Layer} → DirectedPath a → Type (o ⊔ ℓ)
  WeightProduct (path-base _) = Lift (o ⊔ ℓ) ⊤
  WeightProduct (path-step b conn rest) = WeightSpace _ b conn × WeightProduct rest

  -- Extract the output layer from a path
  OutputLayer : {a : Layer} → DirectedPath a → Layer
  OutputLayer {a} (path-base _) = a
  OutputLayer (path-step _ _ rest) = OutputLayer rest

  -- Type for cooperative sum over multiple paths
  CooperativeSumType : (a : Layer) → (paths : List (DirectedPath a)) → Type (o ⊔ ℓ)
  CooperativeSumType a [] = Lift (o ⊔ ℓ) ⊤
  CooperativeSumType a (p ∷ ps) =
    (ActivityManifold a → WeightProduct p → ActivityManifold (OutputLayer p)) ×
    CooperativeSumType a ps

  -- Differential along a path (tangent map composition)
  PathDifferential : {a : Layer} →
                    (path : DirectedPath a) →
                    WeightProduct path →
                    Type (o ⊔ ℓ)
  PathDifferential {a} path w =
    (x : ActivityManifold a) →
    (δw : WeightProduct path) →
    ActivityManifold (OutputLayer path)

  -- Global input at initial layer (for networks with single input)
  GlobalInput : Type (o ⊔ ℓ)
  GlobalInput = Σ[ a ∈ Layer ] (is-input a × ActivityManifold a)

  -- Global weight assignment for entire network
  GlobalWeights : Type (o ⊔ ℓ)
  GlobalWeights = Σ[ a ∈ Layer ] Σ[ b ∈ Layer ] Σ[ conn ∈ Connection a b ] WeightSpace a b conn

  postulate
    -- The map along a path for fixed weights
    φ-path : {a : Layer} →
             (path : DirectedPath a) →
             (weights : WeightProduct path) →
             ActivityManifold a → ActivityManifold (OutputLayer path)

    -- Cooperative sum: combines paths that merge
    cooperative-sum : {a : Layer} →
                     (paths : List (DirectedPath a)) →
                     CooperativeSumType a paths

  {-|
  ## Backpropagation Formula (Lemma 1.1)

  From the paper (Equation 1.10):
  ```
  dξₙ(δw_a) = Σ_{γ_a ∈ Ω_a} Π_{b_k ∈ γ_a} DX^{w₀}_{b_kB_k} ∘ Dρ_{B_kb_{k-1}} ∘ ∂_wX^w_{aA}.δw_a
  ```

  This gives a linear map from T_{w₀}(W_a) to T_{ξ₀}(X_n).

  Composing with dF (the gradient of the loss function) and applying the Riemannian
  metric gives the vector field β(w₀|ξ₀).
  -}

  postulate
    -- Tangent bundle of activity manifold
    TangentActivity : (a : Layer) → ActivityManifold a → Type (o ⊔ ℓ)

    -- Tangent bundle of weight space
    TangentWeight : {a b : Layer} → (conn : Connection a b) →
                   WeightSpace a b conn → Type (o ⊔ ℓ)

    -- Differential of the network map along a path
    D-path-map : {a : Layer} →
                (path : DirectedPath a) →
                (w : WeightProduct path) →
                PathDifferential path w

    -- Backpropagation differential (Lemma 1.1)
    backprop-differential : {a : Layer} →
                           (ξ₀ : GlobalInput) →
                           (w₀ : GlobalWeights) →
                           (x : ActivityManifold a) →
                           TangentActivity a x

  {-|
  ## Theorem 1.1: Backpropagation as Natural Transformation

  From the paper:
  > "Backpropagation is a flow of natural transformations of W, computed from
  > collections of singletons in X."

  The gradient flow β integrates to a one-parameter family of natural transformations
  of the weight presheaf W.
  -}

  postulate
    -- The weight presheaf W : C^op → Sets
    -- (Presheaf on Fork-Category with values in activity/weight spaces)
    WeightPresheaf : Functor (Fork-Category ^op) (Sets (o ⊔ ℓ))

    -- Activity presheaf X^w for fixed weights
    ActivityPresheaf : GlobalWeights → Functor (Fork-Category ^op) (Sets (o ⊔ ℓ))

    -- Natural transformation: gradient flow on weights
    -- This is the key result of Theorem 1.1
    BackpropagationFlow : (w : GlobalWeights) → WeightPresheaf => ActivityPresheaf w

{-|
## Theorem 1.2: The Poset X of a DNN (Section 1.5)

From the paper:

> "The poset X of a DNN is made by a finite number of trees, rooted in the
> maximal points and which are joined in the minimal points."

**Minimal elements** (arrows point TO these):
- Output layers (terminal in Γ)
- Tips a' (vertices that feed into forks)

**Maximal elements** (arrows point FROM these):
- Input layers (initial in Γ)
- Tangs A (fork join points)

**Key property**: After removing A★, we get a poset where the category C_X is
opposite to the free category on the forked graph.
-}
module _ (Γ : OrientedGraph o ℓ) where
  open OrientedGraph Γ

  -- The poset X is ForkVertex minus the stars (Section 1.5, Proposition 1.1)
  data X-Vertex : Type (o ⊔ ℓ) where
    x-original   : Layer → X-Vertex
    x-fork-tang  : (a : Layer) → is-convergent a → X-Vertex

  -- Ordering on X: paths in the forked graph (excluding stars)
  -- Arrows go OPPOSITE to information flow (categorical convention)
  data _≤ˣ_ : X-Vertex → X-Vertex → Type (o ⊔ ℓ) where
    -- Reflexivity
    ≤ˣ-refl : ∀ {x} → x ≤ˣ x

    -- Original edge (non-convergent): y ≤ x (arrow x → y in Γ)
    ≤ˣ-orig : ∀ {x y} → Connection x y → ¬ (is-convergent y) →
              x-original y ≤ˣ x-original x

    -- Tang to handle: a ≤ A (arrow A → a in forked graph)
    ≤ˣ-tang-handle : ∀ {a} (conv : is-convergent a) →
                     x-original a ≤ˣ x-fork-tang a conv

    -- Tip to tang: a' ≤ A (path a' → A★ → A in forked graph)
    ≤ˣ-tip-tang : ∀ {a' a} (conv : is-convergent a) → Connection a' a →
                  x-fork-tang a conv ≤ˣ x-original a'

    -- Transitivity
    ≤ˣ-trans : ∀ {x y z} → x ≤ˣ y → y ≤ˣ z → x ≤ˣ z

  -- Poset axioms (should follow from directed property of Γ)
  postulate
    ≤ˣ-thin : ∀ {x y} → is-prop (x ≤ˣ y)
    ≤ˣ-antisym : ∀ {x y} → x ≤ˣ y → y ≤ˣ x → x ≡ y

  -- The poset X (this is C_X in the paper)
  X-Poset : Poset (o ⊔ ℓ) (o ⊔ ℓ)
  X-Poset = poset where
    open Poset
    poset : Poset (o ⊔ ℓ) (o ⊔ ℓ)
    poset .Ob = X-Vertex
    poset ._≤_ = _≤ˣ_
    poset .≤-thin = ≤ˣ-thin
    poset .≤-refl = ≤ˣ-refl
    poset .≤-trans = ≤ˣ-trans
    poset .≤-antisym = ≤ˣ-antisym

  -- Convert poset to category (standard construction)
  -- Objects: X-Vertex
  -- Hom(x,y): x ≤ y (proposition, so ≤1 morphism)
  X-Category : Precategory (o ⊔ ℓ) (o ⊔ ℓ)
  X-Category = cat where
    open Poset X-Poset
    cat : Precategory (o ⊔ ℓ) (o ⊔ ℓ)
    cat .Precategory.Ob = X-Vertex
    cat .Precategory.Hom x y = x ≤ˣ y
    cat .Precategory.Hom-set x y = is-prop→is-set ≤-thin
    cat .Precategory.id = ≤ˣ-refl
    cat .Precategory._∘_ = flip ≤ˣ-trans
    cat .Precategory.idr _ = ≤-thin _ _
    cat .Precategory.idl _ = ≤-thin _ _
    cat .Precategory.assoc _ _ _ = ≤-thin _ _

  {-|
  ## Alexandrov (Lower) Topology (Section 1.5, Definitions 1)

  From the paper:

  > "The (lower) Alexandrov topology on X is made by subsets U of X such that
  > (y ∈ U and x ≤ y) imply x ∈ U."

  **Basis**: For each α ∈ X, the principal ideal ↓α = { β | β ≤ α }

  **Geometric interpretation**: Information flows "downward" from inputs to outputs.
  In the categorical direction (opposite to graph edges), this is the upward closure.

  **Sheaf condition**: A presheaf F on X is a sheaf iff F(U) ≅ lim_{x ∈ U} F(x)
  for every open U.

  For the DNN topos, the Grothendieck topology is given by covering sieves
  that respect this Alexandrov structure.
  -}
