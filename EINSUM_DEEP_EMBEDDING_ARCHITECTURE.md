# Species ‚Üí Einsum: Deep Embedding Architecture

**Date**: 2025-11-01
**Status**: Foundation laid - Index system + Expression AST created
**Vision**: Typed compilation from combinatorial species to GPU-optimized einsum

---

## üéØ The Big Picture

```
NeuralNet (high-level operations)
    ‚Üì network-to-species
Species (combinatorial structure + symmetries)
    ‚Üì compileSpecies
Einsum Expression (typed AST with index tracking)
    ‚Üì optimize
Einsum Expression (fused/reordered)
    ‚Üì emit-triton
Triton GPU Kernel (Python code)
    ‚Üì execute
Tensor Result
```

**Key insight**: Species capture the **semantics** (what computation means), Einsum captures the **syntax** (how to execute it efficiently).

---

## üìê Type Structure

### Index System (`Einsum/Index.agda`)

**Index**: Named dimension label (String for flexibility)
```agda
Idx : Type
Idx = String

-- Examples
"i", "j", "k"           -- Generic dimensions
"b"                      -- Batch
"t", "s"                 -- Time/sequence
"h", "e"                 -- Heads, embedding (attention)
"c", "i", "o"            -- Channels (conv)
```

**IndexCtx**: List of dimensions (open/extensible product)
```agda
IndexCtx : Type
IndexCtx = List Idx

-- Examples
[i, j]                   -- 2D matrix
[b, t, d]                -- Batch of sequences
[b, h, q, k]             -- Attention scores
```

**Operations**:
- `_‚àà·µ¢_` : Index membership
- `_\\_` : Remove indices (set difference)
- `_‚äé·µ¢_` : Disjoint union
- `_++·µ¢_` : Append
- `dim` : Count dimensions

### Einsum Expression (`Einsum/Expression.agda`)

**Deep embedding** - GADT indexed by input/output contexts:

```agda
data Einsum : (inputs : List IndexCtx) ‚Üí (output : IndexCtx) ‚Üí Type‚ÇÅ where

  -- Contract: Sum over matching indices
  Contract : (contracted : List Idx)
           ‚Üí (remaining : List IndexCtx)
           ‚Üí ...
           ‚Üí Einsum [œÉ‚ÇÅ, œÉ‚ÇÇ] (remaining !! 0 ++ remaining !! 1)

  -- Sequential composition
  Seq : Einsum ins [mid] ‚Üí Einsum [mid] out ‚Üí Einsum ins out

  -- Parallel (fork)
  Par : Einsum ins‚ÇÅ out‚ÇÅ ‚Üí Einsum ins‚ÇÇ out‚ÇÇ
      ‚Üí Einsum (ins‚ÇÅ ++ ins‚ÇÇ) (out‚ÇÅ ++ out‚ÇÇ)

  -- Broadcast (add dimensions)
  Broadcast : (new-dims : List Idx) ‚Üí Einsum [œÉ] (œÉ ++ new-dims)

  -- Reduce (sum over dimension)
  Reduce : (dim : Idx) ‚Üí (dim ‚àà œÉ) ‚Üí Einsum [œÉ] (œÉ \\ [dim])

  -- Transpose (reorder)
  Transpose : Permutation œÉ ‚Üí Einsum [œÉ] (permute-ctx perm œÉ)

  -- Reshape (change shape, preserve size)
  Reshape : (new-shape : IndexCtx) ‚Üí ... ‚Üí Einsum [œÉ] new-shape
```

---

## üîß Concrete Examples

### Matrix Multiplication

**Operation**: `A[i,j] √ó B[j,k] ‚Üí C[i,k]`

**String notation**: `"ij,jk->ik"`

**Deep embedding**:
```agda
MatMul : Einsum [[i, j], [j, k]] [i, k]
MatMul = Contract
  [j]              -- Contract over 'j'
  [[i], [k]]       -- Keep 'i' from A, 'k' from B
  refl ...
```

**Type safety**: Attempting `Contract [x]` on `[[i,j], [j,k]]` would fail because `x ‚àâ both inputs`.

### Dense Layer

**Operation**: `W[i,j] √ó x[j] + b[j] ‚Üí y[i]` (with batch: `x[b,j]`)

**Einsum**: `"bi,ij,j->bj"` (batch-input √ó weights √ó bias)

**Deep embedding**:
```agda
DenseEinsum : Einsum [[batch, in], [in, out], [out]] [batch, out]
DenseEinsum =
  let wx = Contract [in] [[batch], [out]] ...  -- W¬∑x
      add-bias = Broadcast [out] ...            -- Add bias
  in Seq wx add-bias
```

### Conv1D (after im2col)

**Operation**: `x_windows[b,w,k,i] √ó kernel[k,i,o] ‚Üí output[b,w,o]`

**Einsum**: `"bwki,kio->bwo"`

**Deep embedding**:
```agda
ConvEinsum : Einsum
  [[batch, window, kernel-size, in-channels],
   [kernel-size, in-channels, out-channels]]
  [batch, window, out-channels]

ConvEinsum = Contract [kernel-size, in-channels] [[batch, window], [out-channels]] ...
```

### Multi-Head Attention

**Full sequence** (6 einsum operations):

1. **Project Q**: `Q[b,q,d] √ó W_q[d,h,e] ‚Üí Q'[b,q,h,e]`
   `"bqd,dhe->bqhe"`

2. **Project K**: `K[b,k,d] √ó W_k[d,h,e] ‚Üí K'[b,k,h,e]`
   `"bkd,dhe->bkhe"`

3. **Project V**: `V[b,v,d] √ó W_v[d,h,e] ‚Üí V'[b,v,h,e]`
   `"bvd,dhe->bvhe"`

4. **Scores**: `Q'[b,q,h,e] √ó K'·µÄ[b,k,h,e] ‚Üí scores[b,h,q,k]`
   `"bqhe,bkhe->bhqk"`

5. **Apply attention**: `scores[b,h,q,k] √ó V'[b,k,h,e] ‚Üí out[b,q,h,e]`
   `"bhqk,bkhe->bqhe"`

6. **Output proj**: `out_flat[b,q,f] √ó W_out[f,o] ‚Üí final[b,q,o]`
   `"bqf,fo->bqo"`

**Deep embedding**:
```agda
AttentionEinsum : Einsum [...complex inputs...] [batch, seq-q, d-out]
AttentionEinsum =
  let proj-q = Contract [d-model] [[batch, seq-q], [heads, d-head]] ...
      proj-k = Contract [d-model] [[batch, seq-k], [heads, d-head]] ...
      proj-v = Contract [d-model] [[batch, seq-v], [heads, d-head]] ...
      scores = Contract [heads, d-head] [[batch, seq-q], [batch, seq-k]] ...
      apply = Contract [seq-k] [[batch, heads, seq-q], [heads, d-head]] ...
      output = Contract [heads_x_d-head] [[batch, seq-q], [d-out]] ...
  in Seq (Par (Par (Par proj-q proj-k) proj-v) id)
         (Seq scores (Seq apply output))
```

**Non-linear step**: Softmax applied after step 4 (not represented in einsum)

---

## üß¨ Species Integration

### Species Definition

**Species**: Functor `FinSet ‚Üí Type` describing combinatorial structures

```agda
Species : Type‚ÇÅ
Species = Nat ‚Üí Type  -- Structures on n-element sets

-- Operations
_‚äï_ : Species ‚Üí Species ‚Üí Species  -- Coproduct (disjoint union)
_‚äó_ : Species ‚Üí Species ‚Üí Species  -- Product (pairs)
_‚àò‚Çõ_ : Species ‚Üí Species ‚Üí Species  -- Composition
```

### Neural Network ‚Üí Species

```agda
network-to-species : NeuralNet m n ‚Üí Species

-- Examples
network-to-species (Prim (Dense W b)) = DenseSpecies m n
network-to-species (f ‚äô g) = network-to-species f ‚àò‚Çõ network-to-species g
network-to-species (Fork f g) = network-to-species f ‚äï network-to-species g
```

### Species ‚Üí Einsum Compilation

```agda
compileSpecies : (S : Species)
               ‚Üí (weights : WeightSpec S)
               ‚Üí Œ£[ ctx ‚àà (List IndexCtx √ó IndexCtx) ] Einsum (fst ctx) (snd ctx)

-- Examples
compileSpecies (DenseSpecies m n) weights =
  ( [[batch, in], [in, out], [out]]
  , [batch, out]
  , DenseEinsum
  )

compileSpecies (AttentionSpecies d h dh) weights =
  ( [...6 weight matrices...]
  , [batch, seq-q, d-out]
  , AttentionEinsum
  )
```

---

## üöÄ Optimization & Fusion

### Algebraic Laws

**Associativity**:
```agda
(e‚ÇÅ ‚®æ e‚ÇÇ) ‚®æ e‚ÇÉ ‚â° e‚ÇÅ ‚®æ (e‚ÇÇ ‚®æ e‚ÇÉ)
```

**Fusion** (merge consecutive contractions):
```agda
optimize : Einsum ins out ‚Üí Einsum ins out
optimize (Seq (Contract ...) (Contract ...)) = Contract ...  -- Single fused op
```

**Reordering** (for cache locality):
```agda
reorder-for-cache : Einsum ins out ‚Üí Einsum ins out
```

### Compilation Passes

1. **Fusion**: Merge adjacent operations
2. **CSE**: Eliminate common subexpressions
3. **Loop reordering**: Optimize memory access patterns
4. **Kernel selection**: Choose best Triton kernel for pattern

---

## üìù Code Generation

### Python/PyTorch

```agda
emit-python : Einsum ins out ‚Üí String

-- Example
emit-python MatMul
-- ‚áí "torch.einsum('ij,jk->ik', A, B)"

emit-python DenseEinsum
-- ‚áí "torch.einsum('bi,ij,j->bj', x, W, b)"
```

### Triton GPU Kernel

```agda
emit-triton : Einsum ins out ‚Üí String

-- Example (conceptual)
emit-triton AttentionEinsum
-- ‚áí Multi-kernel fused attention with:
--    - Shared memory tiling
--    - Warp-level primitives
--    - Flash attention optimizations
```

---

## üìÅ File Structure

### Created
- ‚úÖ `src/Neural/Compile/Einsum/Index.agda` (~266 lines, 2 holes)
- ‚úÖ `src/Neural/Compile/Einsum/Expression.agda` (~237 lines, created)

### Planned
- ‚è≥ `src/Neural/Compile/Einsum/Eval.agda` - Execution semantics
- ‚è≥ `src/Neural/Compile/Einsum/Optimize.agda` - Fusion & rewriting
- ‚è≥ `src/Neural/Compile/Einsum/Emit.agda` - Code generation
- ‚è≥ `src/Neural/Combinatorial/Species.agda` - Species theory
- ‚è≥ `src/Neural/Combinatorial/SpeciesToEinsum.agda` - Compilation

---

## üéØ Integration Points

### With ForkExtract.agda

```agda
-- Alternative extraction path
extract-via-einsum : NeuralNet m n ‚Üí String  -- Python code
extract-via-einsum net =
  let species = network-to-species net
      einsum = compileSpecies species (extract-weights net)
      optimized = optimize einsum
  in emit-python optimized
```

### With TritonModel.agda

```agda
-- Prove correctness
einsum-fork-equiv : (net : NeuralNet m n)
                  ‚Üí einsum-eval (compileSpecies (network-to-species net))
                  ‚â° fork-exec (extract net)
```

---

## üî¨ Benefits

### 1. Type Safety
**Index mismatches detected at compile time**:
```agda
-- Type error: Can't contract 'k' from inputs that don't contain it
bad-contract : Einsum [[i, j], [m, n]] [i, n]
bad-contract = Contract [k] [[i], [n]] ...  -- ‚ùå Type error
```

### 2. Compositionality
**Einsum expressions compose like neural networks**:
```agda
mlp : Einsum [[batch, in1]] [batch, out2]
mlp = dense1 ‚®æ relu ‚®æ dense2
```

### 3. Optimization
**Algebraic rewriting**:
```agda
-- Fuse two matrix multiplies into single triply-nested loop
(A ¬∑ B) ¬∑ C  ‚üø  optimized-triple-product A B C
```

### 4. Verification
**Prove correctness**:
```agda
eval-preserves-semantics : (e : Einsum ins out)
                          ‚Üí eval e ‚â° denotational-meaning e
```

### 5. Multiple Backends
**Single source, multiple targets**:
- PyTorch (`torch.einsum`)
- NumPy (`np.einsum`)
- JAX (`jax.numpy.einsum`)
- Triton (custom kernels)
- XLA (TensorFlow compiler)

---

## üöß Current Status

**Completed**:
- ‚úÖ Index system with contexts
- ‚úÖ Deep embedding GADT structure
- ‚úÖ Smart constructors for common operations
- ‚úÖ Examples (MatMul, Dense, Conv, Attention)

**Next Steps**:
1. Fill Expression.agda holes (proofs for Contract constructors)
2. Implement Eval.agda (tensor execution semantics)
3. Create Species.agda (basic species theory)
4. Implement SpeciesToEinsum.agda (compilation function)
5. Add Optimize.agda (fusion passes)
6. Add Emit.agda (code generation)

**Timeline**: ~2-3 hours for core infrastructure, then integration with existing modules.

---

## üí° Vision: End-to-End Verified Compilation

```
User writes:
    mlp = Dense 128 784 ‚äô ReLU ‚äô Dense 10 128

Compiler produces:
    torch.einsum('bi,ij->bj',
                 torch.relu(torch.einsum('bi,ij->bj', x, W1)),
                 W2)

Theorem guarantees:
    exec(compile(mlp)) ‚â° ‚ü¶mlp‚üß  (denotational semantics)
```

**The dream**: Write high-level neural network code with mathematical semantics, get optimized GPU kernels with verified correctness!

---

**Session**: 2025-11-01
**Achievement**: Laid foundation for Species ‚Üí Einsum compilation with deep embedding
**Status**: Index + Expression modules created, ready for evaluation semantics
