"""
Tensor Program Encoding using Stack Structure from "Topos of DNNs" Paper

Based on Belfiore & Bennequin (2022): "Topos and Stacks of Deep Neural Networks"

This implementation shows how to encode tensor programs using the stack structure
defined in the paper, particularly focusing on:
1. The fork construction for handling multiple inputs (Section 1.3)
2. The fibered category structure (Chapter 2)
3. The tensor representation of semantic information (Section 3.4)
"""

import numpy as np
import jax.numpy as jnp
from dataclasses import dataclass
from typing import Dict, List, Tuple, Any, Optional, Callable
from abc import ABC, abstractmethod


################################################################################
# PART 1: BASIC CATEGORY AND SITE STRUCTURES
################################################################################

@dataclass
class Layer:
    """Represents a layer in the neural network (vertex in the directed graph)."""
    id: int
    name: str
    dimension: int  # Number of neurons/features
    type: str  # 'input', 'hidden', 'output'


@dataclass
class Edge:
    """Represents a connection between layers (morphism in the category)."""
    source: int  # Source layer id
    target: int  # Target layer id
    weight_shape: Tuple[int, int]


class DirectedGraph:
    """The underlying directed graph Γ of the neural network (Section 1.1)."""

    def __init__(self):
        self.layers: Dict[int, Layer] = {}
        self.edges: List[Edge] = []
        self.partial_order = {}  # a ≤ b if there's a path from a to b

    def add_layer(self, layer: Layer):
        self.layers[layer.id] = layer

    def add_edge(self, edge: Edge):
        self.edges.append(edge)
        # Update partial ordering
        self._update_partial_order(edge.source, edge.target)

    def _update_partial_order(self, source: int, target: int):
        """Maintain the partial ordering relation."""
        if source not in self.partial_order:
            self.partial_order[source] = set()
        self.partial_order[source].add(target)


################################################################################
# PART 2: FORK CONSTRUCTION (Section 1.3)
################################################################################

@dataclass
class Fork:
    """
    Fork construction for handling convergent multiplicity.
    When multiple layers send information to one layer, we introduce a fork.

    From the paper: "between a and a' (resp. a and a'', etc.) introduce two new
    objects A* and A, with arrows a' → A*, a'' → A*, ..., and A* → A, a → A"
    """
    convergence_point: int  # The layer 'a' where inputs converge
    star_node: int  # A* - collects all inputs
    handle_node: int  # A - connects to the convergence point
    input_layers: List[int]  # [a', a'', ...]


class Category:
    """
    The category C(Ɣ) from Section 1.3.
    This is the opposite of the category freely generated by Ɣ (with forks).
    """

    def __init__(self, graph: DirectedGraph):
        self.graph = graph
        self.forks: Dict[int, Fork] = {}
        self._construct_forks()

    def _construct_forks(self):
        """Construct forks for all convergence points."""
        convergence_points = {}

        # Find all convergence points
        for edge in self.graph.edges:
            if edge.target not in convergence_points:
                convergence_points[edge.target] = []
            convergence_points[edge.target].append(edge.source)

        # Create forks for points with multiple inputs
        next_id = max(self.graph.layers.keys()) + 1
        for target, sources in convergence_points.items():
            if len(sources) > 1:
                fork = Fork(
                    convergence_point=target,
                    star_node=next_id,
                    handle_node=next_id + 1,
                    input_layers=sources
                )
                self.forks[target] = fork
                next_id += 2


################################################################################
# PART 3: PRESHEAVES AND FUNCTORS (Section 1.2-1.3)
################################################################################

class Presheaf(ABC):
    """
    Abstract presheaf over the category C.
    Maps objects and morphisms contravariantly to Sets.
    """

    @abstractmethod
    def object_map(self, obj_id: int) -> Any:
        """Map an object in C to a set."""
        pass

    @abstractmethod
    def morphism_map(self, source: int, target: int) -> Callable:
        """Map a morphism in C to a function between sets."""
        pass


class ActivityPresheaf(Presheaf):
    """
    The presheaf X^w representing network activities (Section 1.3).
    For each layer, gives the set of possible neuron activities.
    """

    def __init__(self, category: Category, weights: Dict[Tuple[int, int], jnp.ndarray]):
        self.category = category
        self.weights = weights

    def object_map(self, obj_id: int) -> jnp.ndarray:
        """Returns the activity space at a layer."""
        if obj_id in self.category.graph.layers:
            layer = self.category.graph.layers[obj_id]
            return jnp.zeros(layer.dimension)  # Activity vector
        elif obj_id in [f.star_node for f in self.category.forks.values()]:
            # Fork star node: product of input activities
            fork = next(f for f in self.category.forks.values() if f.star_node == obj_id)
            dims = [self.category.graph.layers[i].dimension for i in fork.input_layers]
            return jnp.zeros(sum(dims))  # Concatenated activities
        else:
            return jnp.zeros(1)

    def morphism_map(self, source: int, target: int) -> Callable:
        """Returns the transmission function along an edge."""
        if (source, target) in self.weights:
            W = self.weights[(source, target)]
            return lambda x: jnp.matmul(W, x)
        else:
            return lambda x: x  # Identity for structural morphisms


class WeightPresheaf(Presheaf):
    """
    The presheaf Π (also denoted W) encoding weights (Section 1.2).
    Π_k is the product of all weight sets for edges l+1,l where l ≥ k.
    """

    def __init__(self, category: Category):
        self.category = category

    def object_map(self, obj_id: int) -> Dict:
        """Returns all downstream weights from this layer."""
        downstream_weights = {}
        # Collect all weights from this layer onwards
        for edge in self.category.graph.edges:
            if edge.source >= obj_id:  # Downstream or at this layer
                downstream_weights[(edge.source, edge.target)] = edge.weight_shape
        return downstream_weights

    def morphism_map(self, source: int, target: int) -> Callable:
        """Forgetting projection: drops weights as we move forward."""
        def forget_weights(weights_dict):
            # Remove weights that are before the target layer
            return {k: v for k, v in weights_dict.items() if k[0] >= target}
        return forget_weights


################################################################################
# PART 4: STACK STRUCTURE (Chapter 2)
################################################################################

@dataclass
class Fiber:
    """
    A fiber in the stack over a layer.
    Represents the local structure (e.g., symmetries, features) at that layer.
    """
    layer_id: int
    local_category: Any  # The category F_U in the paper
    invariance_group: Optional[Any] = None  # For CNNs, translation group


class Stack:
    """
    The stack F → C from Chapter 2.
    A fibered category incorporating symmetries and invariances.

    From the paper: "Invariance structures in the layers (like CNNs or LSTMs)
    correspond to Giraud's stacks."
    """

    def __init__(self, base_category: Category):
        self.base = base_category
        self.fibers: Dict[int, Fiber] = {}
        self._construct_fibers()

    def _construct_fibers(self):
        """Construct fibers for each layer."""
        for layer_id, layer in self.base.graph.layers.items():
            if layer.type == 'conv':
                # CNN layer: fiber has translation group action
                fiber = Fiber(
                    layer_id=layer_id,
                    local_category="TranslationGroupoid",
                    invariance_group="Z^2"  # 2D translations
                )
            else:
                # Regular layer: trivial fiber
                fiber = Fiber(
                    layer_id=layer_id,
                    local_category="TrivialCategory"
                )
            self.fibers[layer_id] = fiber

    def get_fiber(self, layer_id: int) -> Fiber:
        """Get the fiber over a layer."""
        return self.fibers.get(layer_id)

    def transport(self, source: int, target: int, element: Any) -> Any:
        """
        Transport an element along a morphism in the base.
        This implements the equivariance formula (2.1) from the paper.
        """
        # For simplicity, we return the element unchanged
        # In practice, this would apply the appropriate group action
        return element


################################################################################
# PART 5: TENSOR ENCODING OF SEMANTIC INFORMATION (Section 3.4)
################################################################################

class TensorEncoding:
    """
    Encodes semantic information as tensors following Section 3.4.

    From the paper: "The information appears as a tensor F^{δ_1^0,...,δ_k^n}(S)"
    where S is a theory (semantic content) and the δ's are morphisms in the network.
    """

    def __init__(self, stack: Stack):
        self.stack = stack
        self.theories: Dict[str, Any] = {}  # Semantic theories at each layer
        self.propositions: Dict[str, Any] = {}  # Logical propositions

    def encode_theory(self, layer_id: int, theory: Dict[str, Any]) -> jnp.ndarray:
        """
        Encode a semantic theory at a layer as a tensor.

        The theory is a collection of propositions that are stable under
        morphisms to the right (consequences of a discourse).
        """
        fiber = self.stack.get_fiber(layer_id)

        # Dimension depends on the fiber structure
        if fiber.invariance_group == "Z^2":
            # CNN layer: tensor has spatial dimensions
            h, w = 28, 28  # Example spatial dimensions
            c = len(theory)  # Number of semantic channels
            tensor = jnp.zeros((h, w, c))

            # Encode theory as activation patterns
            for i, (prop, value) in enumerate(theory.items()):
                if isinstance(value, bool) and value:
                    tensor = tensor.at[:, :, i].set(1.0)
                elif isinstance(value, float):
                    tensor = tensor.at[:, :, i].set(value)
        else:
            # Regular layer: vector encoding
            dim = len(theory)
            tensor = jnp.array([float(v) if isinstance(v, (bool, float)) else 0.0
                               for v in theory.values()])

        return tensor

    def decode_tensor(self, layer_id: int, tensor: jnp.ndarray) -> Dict[str, Any]:
        """Decode a tensor back to semantic theory."""
        fiber = self.stack.get_fiber(layer_id)

        if fiber.invariance_group == "Z^2":
            # CNN layer: extract semantic channels
            theory = {}
            for i in range(tensor.shape[-1]):
                activation = jnp.mean(tensor[:, :, i])
                theory[f"proposition_{i}"] = float(activation)
        else:
            # Regular layer
            theory = {f"proposition_{i}": float(tensor[i])
                     for i in range(len(tensor))}

        return theory

    def tensor_product(self, tensor1: jnp.ndarray, tensor2: jnp.ndarray) -> jnp.ndarray:
        """
        Compute tensor product for combining semantic information.
        This represents logical conjunction in the semantic space.
        """
        # For simplicity, use outer product
        return jnp.outer(tensor1.flatten(), tensor2.flatten()).reshape(-1)

    def conditioning(self, theory_tensor: jnp.ndarray,
                    proposition_tensor: jnp.ndarray) -> jnp.ndarray:
        """
        Condition a theory on a proposition (S|Y from the paper).
        This is the semantic analog of conditional probability.
        """
        # Hadamard product for element-wise conditioning
        return theory_tensor * proposition_tensor


################################################################################
# PART 6: COMPLETE TENSOR PROGRAM EXAMPLE
################################################################################

class TensorProgram:
    """
    A complete tensor program encoded using the stack structure.
    Combines all the above components to process tensor computations
    through the categorical/topos framework.
    """

    def __init__(self):
        # Build the network structure
        self.graph = DirectedGraph()
        self._build_example_network()

        # Construct the category with forks
        self.category = Category(self.graph)

        # Build the stack with invariances
        self.stack = Stack(self.category)

        # Initialize tensor encoding
        self.tensor_encoding = TensorEncoding(self.stack)

        # Initialize weights
        self.weights = self._initialize_weights()

        # Create presheaves
        self.activity_presheaf = ActivityPresheaf(self.category, self.weights)
        self.weight_presheaf = WeightPresheaf(self.category)

    def _build_example_network(self):
        """Build an example network with multiple paths (ResNet-like)."""
        # Input layer
        self.graph.add_layer(Layer(0, "input", 784, "input"))

        # First hidden layer
        self.graph.add_layer(Layer(1, "hidden1", 256, "hidden"))
        self.graph.add_edge(Edge(0, 1, (256, 784)))

        # Second hidden layer (two parallel paths)
        self.graph.add_layer(Layer(2, "hidden2a", 128, "hidden"))
        self.graph.add_layer(Layer(3, "hidden2b", 128, "hidden"))
        self.graph.add_edge(Edge(1, 2, (128, 256)))
        self.graph.add_edge(Edge(1, 3, (128, 256)))

        # Merge layer (convergence point - will have fork)
        self.graph.add_layer(Layer(4, "merge", 256, "hidden"))
        self.graph.add_edge(Edge(2, 4, (128, 128)))
        self.graph.add_edge(Edge(3, 4, (128, 128)))

        # Output layer
        self.graph.add_layer(Layer(5, "output", 10, "output"))
        self.graph.add_edge(Edge(4, 5, (10, 256)))

    def _initialize_weights(self) -> Dict[Tuple[int, int], jnp.ndarray]:
        """Initialize network weights."""
        weights = {}
        for edge in self.graph.edges:
            shape = edge.weight_shape
            # Xavier initialization
            weights[(edge.source, edge.target)] = jnp.array(
                np.random.randn(*shape) * np.sqrt(2.0 / (shape[0] + shape[1]))
            )

        # Special handling for merge layer weights
        # The merge layer (4) receives from two 128-dim layers and outputs 256-dim
        if 4 in self.category.forks:
            # Create merge weights: (256, 256) to handle concatenated inputs
            weights[('merge', 4)] = jnp.array(
                np.random.randn(256, 256) * np.sqrt(2.0 / 512)
            )

        return weights

    def forward_tensor(self, input_tensor: jnp.ndarray,
                       semantic_context: Optional[Dict] = None) -> Tuple[jnp.ndarray, Dict]:
        """
        Forward pass through the tensor program.

        Args:
            input_tensor: Input data
            semantic_context: Optional semantic information to propagate

        Returns:
            output_tensor: Final output
            semantic_trace: Semantic information at each layer
        """
        # Initialize activations
        activations = {0: input_tensor}

        # Initialize semantic trace
        semantic_trace = {}
        if semantic_context:
            semantic_trace[0] = self.tensor_encoding.encode_theory(0, semantic_context)

        # Process through layers
        for edge in self.graph.edges:
            source, target = edge.source, edge.target

            # Check if this is a convergence point with fork
            if target in self.category.forks:
                fork = self.category.forks[target]
                # Collect inputs from all sources
                inputs = [activations[src] for src in fork.input_layers
                         if src in activations]
                if len(inputs) == len(fork.input_layers):
                    # Concatenate inputs at fork star node
                    combined = jnp.concatenate(inputs, axis=-1)
                    # Use the merge weight matrix
                    W = self.weights.get(('merge', target),
                                        jnp.eye(self.graph.layers[target].dimension,
                                               combined.shape[-1]))
                    activations[target] = jnp.tanh(jnp.matmul(W, combined))

                    # Combine semantic information
                    if semantic_context:
                        sem_inputs = [semantic_trace.get(src, jnp.zeros(1))
                                    for src in fork.input_layers]
                        combined_semantic = sem_inputs[0]
                        for sem in sem_inputs[1:]:
                            combined_semantic = self.tensor_encoding.tensor_product(
                                combined_semantic, sem
                            )
                        semantic_trace[target] = combined_semantic
            else:
                # Regular forward pass
                if source in activations:
                    W = self.weights[(source, target)]
                    activations[target] = jnp.tanh(jnp.matmul(W, activations[source]))

                    # Propagate semantic information
                    if source in semantic_trace:
                        # Transport semantic tensor along morphism
                        semantic_trace[target] = self.stack.transport(
                            source, target, semantic_trace[source]
                        )

        # Return final output and semantic trace
        output = activations.get(5, jnp.zeros(10))  # Output layer
        return output, semantic_trace

    def compile_to_tensor_program(self) -> Dict[str, Any]:
        """
        Compile the network to a tensor program representation.
        This is what gets executed on hardware.
        """
        program = {
            'graph': {
                'vertices': list(self.graph.layers.keys()),
                'edges': [(e.source, e.target) for e in self.graph.edges],
                'forks': {k: {
                    'star': f.star_node,
                    'handle': f.handle_node,
                    'inputs': f.input_layers
                } for k, f in self.category.forks.items()}
            },
            'stack': {
                'fibers': {k: {
                    'type': f.local_category,
                    'invariance': f.invariance_group
                } for k, f in self.stack.fibers.items()}
            },
            'weights': {f"{k[0]}_{k[1]}": v.tolist()
                       for k, v in self.weights.items()},
            'tensor_ops': [
                'matmul',
                'concatenate',
                'tensor_product',
                'conditioning'
            ]
        }
        return program


################################################################################
# PART 7: USAGE EXAMPLE
################################################################################

def main():
    """Demonstrate tensor program encoding using the stack structure."""

    print("=" * 70)
    print("TENSOR PROGRAM ENCODING USING STACK STRUCTURE")
    print("Based on 'Topos and Stacks of Deep Neural Networks'")
    print("=" * 70)

    # Create tensor program
    program = TensorProgram()

    print("\n1. NETWORK STRUCTURE")
    print("-" * 40)
    print(f"Layers: {list(program.graph.layers.keys())}")
    print(f"Edges: {[(e.source, e.target) for e in program.graph.edges]}")
    print(f"Forks (convergence points): {list(program.category.forks.keys())}")

    print("\n2. STACK STRUCTURE")
    print("-" * 40)
    for layer_id, fiber in program.stack.fibers.items():
        print(f"Layer {layer_id}: {fiber.local_category}")
        if fiber.invariance_group:
            print(f"  Invariance: {fiber.invariance_group}")

    print("\n3. TENSOR PROGRAM COMPILATION")
    print("-" * 40)
    compiled = program.compile_to_tensor_program()
    print(f"Compiled program has:")
    print(f"  - {len(compiled['graph']['vertices'])} vertices")
    print(f"  - {len(compiled['graph']['edges'])} edges")
    print(f"  - {len(compiled['graph']['forks'])} forks")
    print(f"  - {len(compiled['weights'])} weight tensors")
    print(f"  - Tensor operations: {', '.join(compiled['tensor_ops'])}")

    print("\n4. FORWARD PASS WITH SEMANTIC INFORMATION")
    print("-" * 40)

    # Create input
    input_data = jnp.ones(784) * 0.5

    # Define semantic context
    semantic_context = {
        'is_digit': True,
        'has_loop': False,
        'stroke_count': 3,
        'confidence': 0.9
    }

    # Run forward pass
    output, semantic_trace = program.forward_tensor(input_data, semantic_context)

    print(f"Input shape: {input_data.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Semantic trace through layers: {list(semantic_trace.keys())}")

    print("\n5. KEY INSIGHTS")
    print("-" * 40)
    print("• The fork construction handles multiple inputs elegantly")
    print("• Stack structure encodes invariances (e.g., translation in CNNs)")
    print("• Tensor encoding preserves semantic information through layers")
    print("• The framework unifies topology, logic, and computation")

    print("\n" + "=" * 70)
    print("This demonstrates how tensor programs can be encoded using the")
    print("categorical/topos framework from the paper, providing a rigorous")
    print("mathematical foundation for deep learning architectures.")
    print("=" * 70)


if __name__ == "__main__":
    main()
