"""
Deep Gradient Flow Analysis for Vectorized TRM Model

This test performs multiple training iterations and inspects:
1. Gradient magnitudes across all components
2. Weight updates after optimization steps
3. Intermediate tensor gradients in the computational graph
4. Whether predicates actually learn from data

Author: Claude Code
Date: October 23, 2025
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from trm_neural_symbolic import TRMNeuralSymbolicSolver
from tqdm import tqdm


def deep_gradient_analysis():
    """Perform deep analysis of gradient flow with multiple iterations."""

    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print(f"Device: {device}")

    # Create model
    print("\n=== Creating TRM Model ===")
    model = TRMNeuralSymbolicSolver(
        num_colors=10,
        num_cycles=3,
        device=device
    ).to(device)

    # Create optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    print(f"Model has {len(model.templates)} templates")
    print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")

    # Create training data with DIVERSE colors to force predicate learning
    # Pattern: mixed colors in input ‚Üí specific transformations in target
    batch_size = 4
    H, W = 5, 5

    print("\n=== Creating Training Data ===")
    print(f"Batch size: {batch_size}")
    print(f"Grid size: {H}√ó{W}")
    print(f"Pattern: Mixed colors (checkerboard-like) ‚Üí Color transformations")
    print(f"This forces predicates to discriminate between different colors")

    # Store initial weights
    print("\n=== Capturing Initial Weights ===")
    initial_weights = {}

    # Encoder
    initial_weights['encoder'] = model.encoder.layer1.weight.data.clone()

    # Refiners
    initial_weights['refine_z'] = model.refine_z.refine[0].weight.data.clone()
    initial_weights['refine_y'] = model.refine_y.refine[0].weight.data.clone()

    # Selector
    initial_weights['selector'] = model.formula_selector.scorer[0].weight.data.clone()

    # Predicates (color_eq)
    color_eq_pred = model.predicates_vectorized.get('color_eq')
    initial_weights['color_eq_embed'] = color_eq_pred.color_embed.weight.data.clone()
    initial_weights['color_eq_net'] = color_eq_pred.compare_net[0].weight.data.clone()

    print("‚úÖ Initial weights captured")

    # Training loop
    num_iterations = 50
    print(f"\n=== Training for {num_iterations} Iterations ===")

    losses = []
    grad_mags = {
        'encoder': [],
        'refine_z': [],
        'refine_y': [],
        'selector': [],
        'color_eq_embed': [],
        'color_eq_net': []
    }

    for iteration in tqdm(range(num_iterations), desc="Training"):
        # Create batch with MIXED colors (checkerboard pattern)
        # This forces predicates to actually learn color discrimination
        input_grid = torch.zeros((batch_size, H, W), device=device, dtype=torch.long)
        target_grid = torch.zeros((batch_size, H, W), device=device, dtype=torch.long)

        for b in range(batch_size):
            for i in range(H):
                for j in range(W):
                    # Checkerboard pattern with multiple colors
                    input_grid[b, i, j] = (i + j) % 5  # Colors 0-4
                    # Target: transform color 2 ‚Üí 7, others stay same
                    if input_grid[b, i, j] == 2:
                        target_grid[b, i, j] = 7
                    else:
                        target_grid[b, i, j] = input_grid[b, i, j]

        # Zero gradients
        optimizer.zero_grad()

        # Forward pass
        loss, info = model.compute_loss(input_grid, target_grid)

        # Backward pass
        loss.backward()

        # Collect gradient magnitudes
        grad_mags['encoder'].append(
            model.encoder.layer1.weight.grad.abs().sum().item()
            if model.encoder.layer1.weight.grad is not None else 0.0
        )

        grad_mags['refine_z'].append(
            model.refine_z.refine[0].weight.grad.abs().sum().item()
            if model.refine_z.refine[0].weight.grad is not None else 0.0
        )

        grad_mags['refine_y'].append(
            model.refine_y.refine[0].weight.grad.abs().sum().item()
            if model.refine_y.refine[0].weight.grad is not None else 0.0
        )

        grad_mags['selector'].append(
            model.formula_selector.scorer[0].weight.grad.abs().sum().item()
            if model.formula_selector.scorer[0].weight.grad is not None else 0.0
        )

        # Critical: Check color_eq predicate gradients
        color_eq_embed_grad = color_eq_pred.color_embed.weight.grad
        color_eq_net_grad = color_eq_pred.compare_net[0].weight.grad

        grad_mags['color_eq_embed'].append(
            color_eq_embed_grad.abs().sum().item()
            if color_eq_embed_grad is not None else 0.0
        )

        grad_mags['color_eq_net'].append(
            color_eq_net_grad.abs().sum().item()
            if color_eq_net_grad is not None else 0.0
        )

        # Optimizer step
        optimizer.step()

        # Store loss
        losses.append(loss.item())

    print("‚úÖ Training complete")

    # Analyze results
    print("\n" + "="*80)
    print("GRADIENT FLOW ANALYSIS")
    print("="*80)

    print("\n=== Loss Over Time ===")
    print(f"Initial loss: {losses[0]:.4f}")
    print(f"Final loss: {losses[-1]:.4f}")
    print(f"Change: {losses[-1] - losses[0]:.4f}")

    if losses[-1] < losses[0]:
        print("‚úÖ Loss DECREASED - model is learning!")
    else:
        print("‚ùå Loss INCREASED or FLAT - model not learning")

    print("\n=== Gradient Magnitudes (Mean over iterations) ===")

    for component, mags in grad_mags.items():
        mean_grad = sum(mags) / len(mags)
        max_grad = max(mags)
        nonzero_count = sum(1 for m in mags if m > 1e-10)

        status = "‚úÖ" if mean_grad > 1e-10 else "‚ùå"
        print(f"{status} {component:20s}: mean={mean_grad:.6e}, max={max_grad:.6e}, nonzero={nonzero_count}/{len(mags)}")

    print("\n=== Weight Changes ===")

    # Check if weights actually changed
    weight_changes = {}

    # Encoder
    encoder_diff = (model.encoder.layer1.weight.data - initial_weights['encoder']).abs().sum().item()
    weight_changes['encoder'] = encoder_diff
    print(f"{'‚úÖ' if encoder_diff > 1e-10 else '‚ùå'} Encoder: {encoder_diff:.6e}")

    # Refiners
    refine_z_diff = (model.refine_z.refine[0].weight.data - initial_weights['refine_z']).abs().sum().item()
    weight_changes['refine_z'] = refine_z_diff
    print(f"{'‚úÖ' if refine_z_diff > 1e-10 else '‚ùå'} Refine_z: {refine_z_diff:.6e}")

    refine_y_diff = (model.refine_y.refine[0].weight.data - initial_weights['refine_y']).abs().sum().item()
    weight_changes['refine_y'] = refine_y_diff
    print(f"{'‚úÖ' if refine_y_diff > 1e-10 else '‚ùå'} Refine_y: {refine_y_diff:.6e}")

    # Selector
    selector_diff = (model.formula_selector.scorer[0].weight.data - initial_weights['selector']).abs().sum().item()
    weight_changes['selector'] = selector_diff
    print(f"{'‚úÖ' if selector_diff > 1e-10 else '‚ùå'} Selector: {selector_diff:.6e}")

    # Predicates (CRITICAL!)
    color_eq_embed_diff = (color_eq_pred.color_embed.weight.data - initial_weights['color_eq_embed']).abs().sum().item()
    weight_changes['color_eq_embed'] = color_eq_embed_diff
    print(f"{'‚úÖ' if color_eq_embed_diff > 1e-10 else '‚ùå'} ColorEq Embeddings: {color_eq_embed_diff:.6e}")

    color_eq_net_diff = (color_eq_pred.compare_net[0].weight.data - initial_weights['color_eq_net']).abs().sum().item()
    weight_changes['color_eq_net'] = color_eq_net_diff
    print(f"{'‚úÖ' if color_eq_net_diff > 1e-10 else '‚ùå'} ColorEq Network: {color_eq_net_diff:.6e}")

    # Final verdict
    print("\n" + "="*80)
    print("FINAL VERDICT")
    print("="*80)

    all_components_updated = all(
        change > 1e-10 for change in weight_changes.values()
    )

    predicates_updated = (
        weight_changes['color_eq_embed'] > 1e-10 or
        weight_changes['color_eq_net'] > 1e-10
    )

    if all_components_updated and predicates_updated:
        print("‚úÖ SUCCESS: All components received gradients and updated!")
        print("‚úÖ Predicates ARE learning (weights changed)")
        print("‚úÖ Vectorization is working correctly")
        print("\nüéâ Phase 3 Complete - Ready for full training!")
        return True
    elif predicates_updated:
        print("‚ö†Ô∏è  PARTIAL SUCCESS:")
        print("‚úÖ Predicates ARE learning (weights changed)")
        print("‚ùå Some other components didn't update")
        return False
    else:
        print("‚ùå FAILURE: Predicates NOT learning")
        print("‚ùå Vectorization may have issues")
        print("\nDebugging needed:")
        print("1. Check if color_eq predicate is used in selected templates")
        print("2. Verify computational graph connectivity")
        print("3. Inspect intermediate tensor gradients")
        return False


def inspect_computational_graph():
    """Inspect the computational graph for gradient flow."""

    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    print("\n" + "="*80)
    print("COMPUTATIONAL GRAPH INSPECTION")
    print("="*80)

    # Create model
    model = TRMNeuralSymbolicSolver(
        num_colors=10,
        num_cycles=3,
        device=device
    ).to(device)

    # Simple test case
    input_grid = torch.full((1, 3, 3), 2, device=device, dtype=torch.long)
    target_grid = torch.full((1, 3, 3), 7, device=device, dtype=torch.long)

    # Forward pass with hooks to capture intermediate tensors
    intermediate_grads = {}

    def save_grad(name):
        def hook(grad):
            intermediate_grads[name] = grad.abs().sum().item()
        return hook

    # Register hooks
    print("\nRegistering gradient hooks...")

    # Run forward pass
    loss, info = model.compute_loss(input_grid, target_grid)

    # Get output and register hook
    output = info.get('output_grid', None)

    print(f"\nLoss: {loss.item():.4f}")
    print(f"Loss requires_grad: {loss.requires_grad}")
    print(f"Loss grad_fn: {loss.grad_fn}")

    # Backward
    loss.backward()

    # Check which templates were selected
    if 'selected_templates' in info:
        print(f"\nSelected templates:")
        for i, template in enumerate(info['selected_templates']):
            print(f"  {i}: {template}")

    # Check predicate gradients
    color_eq_pred = model.predicates_vectorized.get('color_eq')

    print("\n=== Predicate Gradient Check ===")
    print(f"color_eq.color_embed.weight.grad: {color_eq_pred.color_embed.weight.grad is not None}")
    if color_eq_pred.color_embed.weight.grad is not None:
        print(f"  Magnitude: {color_eq_pred.color_embed.weight.grad.abs().sum().item():.6e}")

    print(f"color_eq.compare_net[0].weight.grad: {color_eq_pred.compare_net[0].weight.grad is not None}")
    if color_eq_pred.compare_net[0].weight.grad is not None:
        print(f"  Magnitude: {color_eq_pred.compare_net[0].weight.grad.abs().sum().item():.6e}")


if __name__ == "__main__":
    print("="*80)
    print("DEEP GRADIENT FLOW ANALYSIS")
    print("="*80)

    # Test 1: Deep gradient analysis over multiple iterations
    success = deep_gradient_analysis()

    # Test 2: Inspect computational graph
    inspect_computational_graph()

    print("\n" + "="*80)
    print("SUMMARY")
    print("="*80)

    if success:
        print("‚úÖ Vectorization is working correctly")
        print("‚úÖ All components learning (including predicates)")
        print("‚úÖ Ready to run full training")
        print("\nNext: python train_trm_neural_symbolic.py")
    else:
        print("‚ùå Issues detected with gradient flow")
        print("‚ùå Need further debugging")
        print("\nCheck:")
        print("1. Are templates using color_eq predicate?")
        print("2. Is vectorized interpreter being called?")
        print("3. Are gradients flowing through _apply_formula_vectorized?")
